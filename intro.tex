In machine learning (ML) we learn broad trends and patterns from vast sums of data. What type of data is collected and how it is used introduces different kinds of privacy risks. Medical data allows us to link characteristics like genetic markers to adverse health conditions like cancer. However it suffers from unique correlation risks, since one's information can reliably be correlated from their familys'. Document embeddings---useful vector representations of text documents---are tremendously useful for inferring general document characteristics like topic, but can expose detailed personal information at the sentence level. Generative vision models can sample novel images, but also tend to copy and expose their training images in clever and inexact ways. 

The privacy risks and utility requirements of each of these settings and applications warrant different approaches to privacy-preserving ML. This dissertation proposes a variety of solutions to situations like those above. In doing so, I hope to illuminate the advantage of taking an application-specific approach to both measuring privacy risks and engineering private algorithms. The following five chapters are roughly organized into two parts: the first two chapters cover \emph{empirical} privacy methods, and the remaining three cover \emph{formal} privacy methods. The former includes statistical tests to quantify privacy risks of large ML models. The latter proposes provably private algorithms to satisfy different privacy definitions chosen for different ML tasks. 

\textbf{Empirical methods.} The first two chapters explore empirically measuring privacy risks in the domain of vision modeling. In both chapters, we analyze to what extent large vision models \emph{memorize} their training images, and thereby risk exposing them. In contrast with the following three chapters, we are not proposing a provably privacy-preserving algorithm. Instead we are designing methodical empirical tests to quantify memorization. 

\textbf{Provably private algorithms.} The final three chapters propose algorithms that allow us to share our data in a provably private way. We explore privacy preserving algorithms in the text, location, and interpersonal-correlated domains (\emph{e.g.} social networks or genetically linked medical data). For each of these, we study different ML tasks and privacy risks to motivate different privacy definitions and provably private algorithms. 

Taken together, this document offers a mindset towards practicable privacy methods. By directly considering the data domain and the task at hand, it is possible to efficiently measure privacy risks and propose provably private algorithms while still completing the learning task at hand .