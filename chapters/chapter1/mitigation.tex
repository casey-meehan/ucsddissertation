\vspace{-.5em} 
\section{Mitigation of \dejavu memorization}
\label{sec:mitigation}
% We do not have an understanding on why \dejavu occur so strongly in some SSL pretraining, however we present additional experiments that shed light on which parameters have the biggest impact on \dejavu memorization.
\input{chapters/chapter1/all_mitigations}
We cannot yet make claims on why \dejavu occurs so strongly for some SSL training settings and not for others. To gain some intuition for future work, we present additional observations that shed light on which parameters have the most salient impact on \dejavu memorization.
\vspace{-.75em}
\paragraph{Déjà vu memorization worsens by increasing number of training epochs.} 
Figure \ref{fig:dejavu v. training epochs} shows how \dejavu memorization changes with number of training epochs for VICReg. The training set size is fixed to 300K samples. From 250 to 1000 epochs, the \dejavu score (red curve) grows \emph{threefold}: from under 10\% to over 30\%. Remarkably, this trend in memorization is \emph{not} reflected by the linear probe gap (dark blue), which only changes by a few percent beyond 250 epochs. 

%\vspace{-.75em}
\paragraph{Training set size has minimal effect on \dejavu memorization.} Figure \ref{fig:dejavu v. n} shows how \dejavu memorization responds to the model's training set size. The number of training epochs is fixed to 1000. Interestingly, training set size appears to have almost \emph{no} influence on the \dejavu score (red line), indicating that memorization is equally prevalent with a 100K dataset and a 500K dataset. This result suggests that \dejavu memorization may be detectable even for large datasets. Meanwhile, the standard linear probe train-test accuracy gap \emph{declines} by more than half as the dataset size grows, failing to represent the memorization quantified by our test. 
% The trend is completely different according to linear probe accuracy (dark blue line), the train-test gap shrinks substantially when increasing the training set size from 100K to 500K. This highlights that the train-test gap is not able to capture \dejavu memorization. Our evidence suggests that \dejavu memorization may be detectable even for large-scale training datasets. 
\vspace{-0.5em}
\paragraph{Training loss hyper-parameter has a strong effect.} 
%We show in Figure \ref{fig:dejavu v. training epochs} that the number of training epochs is an important factor that can increase significantly \dejavu memorization. In contrast, the dataset size does not impact much \dejavu as shown in Figure \ref{fig:dejavu epochs train set size}. 
Loss hyper-parameters, like VICReg's invariance coefficient (Figure \ref{fig:dejavu v. invariance}) or SimCLR's temperature parameter (Appendix Figure \ref{fig:simclr temperature}) significantly impact \dejavu with minimal impact on the linear probe validation accuracy.

\vspace{-0.5em}
\paragraph{Some SSL criteria promote stronger \dejavu memorization.} Table \ref{tab:dejavu vs. criterion} demonstrates that the degree of memorization varies widely for different training criteria. VICReg and Barlow Twins have the highest \dejavu scores while SimCLR and Byol have the lowest.
%\footnote{We show detailed reconstructions of SimCLR's training data in Section \ref{sec:visualizing} despite its relatively low degree of \dejavu.}.
With the exception of Byol, all SSL models have more \dejavu memorization than the supervised model. Interestingly, different criteria can lead to similar linear probe validation accuracy and very different degrees of \dejavu as seen with SimCLR and Barlow Twins. Note that low degrees of \dejavu can still risk training image reconstruction, as exemplified by the SimCLR reconstructions in Figures \ref{fig:mem v corr dam} and \ref{fig:mem v corr spider}. 
%\vspace{-1em}
\vspace{-0.5em}
\paragraph{Larger models have increased \dejavu memorization.} Figure \ref{fig:dejavu v. capacity} validates the common intuition that lower capacity architectures (Resnet18/34) result in less memorization than their high capacity counterparts (Resnet50/101). 
% \begin{wrapfigure}{r}{0.25\textwidth} 
%     \centering
%     \includegraphics[width=0.25\textwidth]{figures/attk_layer_acc_top1_legend.pdf}
%     \caption{\dejavu memorization versus layer from backbone (0) to projector output (3).}
%     \label{fig:dejavu vs layer}
%     \vspace{-8ex}
% \end{wrapfigure}
We see the same trend for vision transformers as well. %This comes with a tradeoff, since reduced model capacity can result in a nontrivial degradation of representation quality\cite{vicreg, simclr}.  
\vspace{-0.5em}
\paragraph{Guillotine regularization can help reduce \dejavu memorization.} Previous experiments were done using the projector embedding. In Figure \ref{fig:dejavu v. guillotine}, we present how Guillotine regularization\citep{Guillotine} (removing final layers in a trained SSL model) impacts \dejavu with VICReg\footnote{Further experiments are available in Appendix \ref{sec:guillotine}.}. Using the backbone embedding instead of the projector embedding seems to be the most straightforward way to mitigate \dejavu memorization. However, as demonstrated in Appendix \ref{sec:appx backbone results}, backbone representation with low \dejavu score can still be leveraged to reconstruct some of the training images.

\section{Conclusion}
\label{sec:conclusion}

We defined and analyzed \dejavu memorization, a notion of unintended memorization of partial information in image data. As shown in Sections \ref{sec:quant} and \ref{sec:visualizing}, SSL models can largely exhibit \dejavu memorization on their training data, and this memorization signal can be extracted to infer or visualize image-specific information.
Since SSL models are becoming increasingly widespread as foundation models for image data, negative consequences of \dejavu memorization can have profound downstream impact and thus deserves further attention. 
Future work should focus on understanding how \dejavu emerges in the training of SSL models and why methods like Byol are much more robust to \dejavu than VICReg and Barlow Twins. In addition, trying to characterize which data points are the most at risk of \dejavu could be crucial to get a better understanding on this phenomenon. 
