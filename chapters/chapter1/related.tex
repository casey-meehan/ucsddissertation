\section{Preliminaries and Related Work}
\label{sec:related}

\textbf{Self-supervised learning} (SSL) is a machine learning paradigm that leverages unlabeled data to learn representations. Many SSL algorithms rely on \emph{joint-embedding} architectures (\emph{e.g.}, SimCLR~\citep{chen2020simclr}, Barlow Twins~\citep{zbontar2021barlow}, VICReg~\citep{vicreg} and Dino~\citep{Dino}), which are trained to associate different augmented views of a given image. For example, in SimCLR, given a set of images $\calA = \{A_1,\ldots,A_n\}$ and a randomized augmentation function $\mathrm{aug}$, the model is trained to maximize the cosine similarity of draws of $\SSL(\mathrm{aug}(A_i))$ with each other and minimize their similarity with $\SSL(\mathrm{aug}(A_j))$ for $i \neq j$. The augmentation function $\mathrm{aug}$ typically consists of operations such as cropping, horizontal flipping, and color transformations to create different views that preserve an image's semantic properties. 

\paragraph{SSL representations.} Once an SSL model is trained, its learned representation can be transferred to different downstream tasks. This is often done by extracting the representation of an image from the \emph{backbone model}\footnote{SSL methods often use a trick called \emph{guillotine regularization}~\citep{Guillotine}, which decomposes the model into two parts: a \emph{backbone model} and a \emph{projector} consisting of a few fully-connected layers. Such trick is needed to handle the misalignment between the pretext SSL task and the downstream task.} and either training a linear probe on top of this representation or finetuning the backbone model with a task-specific head~\citep{Guillotine}.
%Compared to representations learned by supervised learning, SSL representations are often more robust and transferable~\citep{hendrycks2019using, ericsson2021self}, leading to state-of-the-art result on many downstream tasks. To understand the effectiveness of SSL algorithms, several prior works investigated what kind of information the SSL model has learned~\citep{jing2021understanding, ericsson2021self, kalibhat2022towards, RCDM}. In particular, \citet{RCDM} trained a conditional generative model on SSL representations and showed that they encode richer visual details about the input image compared to supervised learning. 
%However, from a privacy perspective, this may be a cause for concern as the model also has more potential to overfit and memorize precise details about the training data compared to supervised learning. We show concretely that this privacy risk can indeed be realized by defining and measuring \emph{déjà vu} memorization.
It has been shown that SSL representations encode richer visual details about input images than supervised models do \cite{RCDM}. However, from a privacy perspective, this may be a cause for concern as the model also has more potential to overfit and memorize precise details about the training data compared to supervised learning. We show concretely that this privacy risk can indeed be realized by defining and measuring \emph{déjà vu} memorization.
\vspace{-0.5em} 
% \paragraph{Privacy risks in ML.} Overfitting in ML occurs when a model memorizes information specific to its training data rather than general population-level information. When the model is trained on privacy-sensitive data, overfitting is especially harmful as an adversary can infer private information about the training data when given access to the model~\citep{yeom2018privacy, feldman2020does}. The simplest and most well-studied form of privacy risk in ML is susceptibility to \emph{membership inference attacks}~\citep{shokri2017membership, salem2018ml, sablayrolles2019white}, where the adversary infers whether an individual is part of the training set or not. More sophisticated privacy attacks include \emph{attribute inference}~\citep{fredrikson2014privacy, mehnaz2022your, jayaraman2022attribute}, where specific attributes about an individual are inferred given others, and \emph{data reconstruction}~\citep{carlini2021extracting, balle2022reconstructing, guo2022bounding}, where entire training samples are recovered from the trained model. Our study of \emph{déjà vu} memorization is similar to both attribute inference and data reconstruction, leveraging SSL representations of the training image background to infer and reconstruct the foreground object.
% \vspace{-0.5em} 
% \paragraph{Training data extraction in NLP.} Our study of \dejavu memorization in SSL models is inspired by similar work in the natural language processing (NLP) domain. \citet{carlini2019secret} first showed that language models exhibit unintended memorization, where given a context string present in its training data, the model can generate the remaining text at test time. This unintended memorization has been further exploited in \citet{carlini2021extracting} to extract training data from GPT-2~\citep{radford2019language} and, more recently, extended to extract memorized images from Stable Diffusion \citep{google_diffusion}. The way by which these works exploit unintended memorization is similar to ours: given partial information about a training sample, the model is prompted to reveal the rest of the sample. In our case, however, since the SSL model is not generative, extraction is significantly harder and requires careful design.

\paragraph{Privacy risks in ML.} When a model is overfit on privacy-sensitive data, it memorizes specific information about its training examples, allowing an adversary with access to the model to learn private information~\citep{yeom2018privacy, feldman2020does}. Privacy attacks in ML range from the simplest and best-studied \emph{membership inference attacks}~\citep{shokri2017membership, salem2018ml, sablayrolles2019white} to \emph{attribute inference}~\citep{fredrikson2014privacy, mehnaz2022your, jayaraman2022attribute} and \emph{data reconstruction}~\citep{carlini2021extracting, balle2022reconstructing, guo2022bounding} attacks. In the former, the adversary only infers whether an individual participated in the training set. Our study of \emph{déjà vu} memorization is most similar to the latter: we leverage SSL representations of the training image background to infer and reconstruct the foreground object. Our approach reflects similar work in the NLP domain \citep{carlini2019secret, carlini2021extracting}: when prompted with a context string present in the training data, a large language model is shown to generate the remainder of string at test time, revealing sensitive text like home addresses. This method was recently extended to extract memorized images from Stable Diffusion \citep{google_diffusion}.  We exploit memorization in a similar manner: given partial information about a training sample, the model is prompted to reveal the rest of the sample. In our case, however, since the SSL model is not generative, extraction is significantly harder and requires careful design.
