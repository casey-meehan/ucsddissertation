% \begin{figure*}[h]
%     \centering
%     \includegraphics[width = 0.85\textwidth]{figures/SSL_attack_cartoon.png}
%     \caption{We measure memorization by comparing the `target model' trained on the target image ($\SSL_A$ trained on $A_i$ in above example) with the `reference model' not trained on it ($\SSL_B$, above). \textbf{[Top Strip]} A cropping of the image disjoint from the labeled foreground object is embedded using the target model. This embedding is then labeled by a K-Nearest Neighbor (KNN) adversary built on a public set of labeled images, $X$, which it has also embedded using the target model. \textbf{[Bottom Strip]} To account for correlation, the same procedure is followed with the reference model. If the label is only extracted using the target model, it is counted as memorization. If it is extracted using either model, it is counted as correlation. We find that the KNN adversary's predictions using the target model (trained on attacked examples) are significantly more accurate than they are using the reference model, indicating routine memorization of training examples.}
%     \label{fig:ssl attack cartoon}
% \end{figure*}

\begin{figure}[t]
%%%
%SPIDER
%%%
     % \centering
     % \begin{subfigure}[b]{0.25\textwidth}
     %     \centering
     %     \includegraphics[width=\textwidth]{figures/data_split.png}
     %     % \caption{SimCLR correlated \textit{yellow garden spider} examples}
     %     \label{fig:data split}
     % \end{subfigure}
     % \hfill
     % \begin{subfigure}[b]{0.7\textwidth}
     %     \centering
     %     \includegraphics[width=\textwidth]{figures/pipeline_cartoon.png}
     %     \begin{minipage}{5cm}
     %        \vfill
     %    \end{minipage}
     %     % \caption{SimCLR memorized \textit{yellow garden spider} examples}
     %     \label{fig:pipeline cartoon}
     % \end{subfigure}
     \includegraphics[width=\textwidth]{figures/split_and_pipeline_cartoon.png}
\caption[Overview of testing methodology.]{
Overview of testing methodology. \textbf{Left:} Data is split into \emph{target set} $\calA$, \emph{reference set} $\calB$ and \emph{public set} $\calX$ that are pairwise disjoint. $\calA$ and $\calB$ are used to train two SSL models $\SSL_A$ and $\SSL_B$ in the same manner. $\calX$ is used for KNN decoding or for training an RCDM to reconstruct the input at test time. \textbf{Right:} Given a training image $A_i \in \calA$, we use $\SSL_A$ to embed $\crop{A_i}$ containing only the background, as well as the entire set $\calX$ and find the $k$-nearest neighbors of $\crop{A_i}$ in $\calX$ in the embedding space. These KNN samples can be used directly to infer the foreground object (\emph{i.e.}, class label) in $A_i$ using a KNN classifier, or their embeddings can be averaged as input to the trained RCDM to visually reconstruct the image $A_i$. For instance, the RCDM reconstruction results in Figure \ref{fig:black_swan} (left) when given $\SSL_A(\crop{A_i})$ and results in Figure \ref{fig:black_swan} (right) when given $\SSL_A(\crop{B_i})$ for an image $B_i \in \calB$.
%\textbf{Left:} illustration of the three datasets used in our tests. Two private data sets, $A$ and $B$, of equal size are used to train two SSL models, $\SSL_A$ and $\SSL_B$, respectively. A disjoint public set, $X$, is made available to the memorization test to help decode model embeddings. Memorization is only tested on examples $A_i \in A$ that are unique to set $A$. \textbf{Right:} illustration of inference pipeline used in tests. A periphery cropping that excludes the foreground object is taken from private image $A_i$. The KNN then finds the $k$ public set nearest neighbors of the periphery crop in the embedding space of $\SSL_A$. 
%The $\SSL_A$ representation of these $k$ neighbors and of the crop are used by the conditional generative model, RCDM, to reconstruct the foreground object. The labels of these $k$ neighbors are used to recover the foreground object label. (Not pictured) We repeat this process using reference model $\SSL_B$, not trained on image $A_i$, to determine whether the foreground object is still recoverable by learned correlations, e.g. if black swans were the only objects appearing near water in the data distribution. In this instance, the crop's public set neighbors in $\SSL_B$'s representation space include a variety of water animals like ducks, pelicans, and otters. Meanwhile, with $\SSL_A$, the neighbors are nearly all black swans in the same position as the swan of $A_i$.
}
\label{fig:split_and_pipeline_cartoon}
\end{figure}