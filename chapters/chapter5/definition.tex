\section{Conditional Inferential Privacy}
We now propose a privacy framework that is tailored to sequences of correlated data, Conditional Inferential Privacy (CIP). CIP guarantees a radius $r$ of indistinguishability for the basic or compound secrets associated with any secret set $\Is$. Specifically, CIP protects against any adversary with a specific prior on \emph{the shape} of the trace, and is agnostic to their prior on the absolute location of the trace. We call the set of such prior distributions a Conditional Prior Class.

\begin{definition} \emph{Conditional Prior Class}
\label{def: conditional prior class}
%	For $X = \{X_1, \dots, X_n\}$, prior distributions $\calP_i, \calP_j$ on $X$ are said to belong to the same conditional prior class $\Theta$ if the distribution of any subset of $X$ conditioned on the rest of $X$ is identical up to a mean shift. Formally, if conditional distributions $P_{\calP_i}(\Xu | \Xs = x_s) = P_{\calP_j}(c_{ij\Is} + \Xu | \Xs = x_s)$ for all $x_s$. Constant $c_{ij\Is}$ may change for different $\calP_i,\calP_j$ and partitions $\Is, \Iu$.
	For $X = \{X_1, \dots, X_n\}$, prior distributions $\calP_i, \calP_j$ on $X$ are said to belong to the same conditional prior class $\Theta$ if a constant shift in the conditioned $x_s$ results in a constant shift on the distribution of $\Xu$. Formally, if conditional distributions $P_{\calP_i}(\Xu | \Xs = x_s) = P_{\calP_j}(\Xu + c_{ij\Is}^u  | \Xs = x_s + c_{ij\Is}^s )$ for all $x_s$.
\end{definition}

For instance, prior $P_{\calP_i}$ may concentrate probability on traces passing through Los Angeles, while $P_{\calP_j}$ concentrates on traces passing through London. Conditioning on each secret in the pair $(x_s, x_s')$ in L.A. is analogous to conditioning on each secret in the pair $(x_s + c_{ij\Is}^s, x_s' + c_{ij\Is}^s)$ in London. The corresponding pair of conditional distributions on $\Xu$ in London ($P_{\calP_j}$) are copies of those in L.A. ($P_{\calP_i}$) shifted by $c_{ij\Is}^u$. What matters is that the set of all pairs of conditional distributions under $P_{\calP_i}$ induced by secret pairs $(x_s, x_s')$ is identical to those under $P_{\calP_j}$ up to a mean shift. See Appendix \ref{apx: GP prior class} for a more detailed discussion of conditional prior classes.  
 
% Two prior distributions $\calP_1$ and $\calP_2$ may have completely different marginal distributions on any $X_i$ but still belong to the same conditional prior class. For instance, $\calP_1$ could concentrate probability on the event that the middle point in the trace passes through Manhattan, and $\calP_2$ could have high uncertainty over which state in the U.S. the trace is in. As long as they have the same prior on the dependence between points, the trace is protected by the same CIP mechanism. 

\begin{definition} \emph{ $(\varepsilon, \lambda)$-Conditional Inferential Privacy $(\Spairs, r, \Theta)$}
	Given compound or basic discriminative pairs $\Spairs$ associated with $\Is$, a radius of privacy $r$, a conditional prior class, $\Theta$, and a privacy parameter, $\varepsilon > 0$, a privacy mechanism $Z = \calA(X)$ satisfies $(\varepsilon, \lambda)$-CIP$(\Spairs, r, \Theta)$ if for all $(s_i, s_j) \in \Spairs$, 
%	where 
%	\begin{align*}
%	\Spairs = \Big\{\big( \{x_{s1}, x_{s2}, \dots\}&, \{x_{s1}', x_{s2}', \dots\}\big) \\
%	 &: \| x_{sk} - x_{sk}' \|_2 \leq 2r, \forall \ k\Big\} 
%	\end{align*} 
	and all prior distributions $\calP \in \Theta$, where $P_\calP(s_i), P_\calP(s_j) > 0$, 
	\begin{align}
		\label{eqn:CIP loss}
		D_\lambda \binom{P_{\calA, \calP} (Z | \Xs = s_i)}{P_{\calA, \calP} (Z | \Xs = s_j)} &\leq \varepsilon
	\end{align}
\end{definition}

%Comparison with DP and PF
CIP departs from DP type notions of privacy like Approaches A$\rightarrow$C primarily by resisting only a restricted class of inter-dependence --- the conditional prior class --- as opposed to arbitrary dependence of any $k$ locations. Unlike approaches A and B, we are able to preserve utility for tasks like traffic monitoring. Unlike approach C, CIP is still resistant to realistic models of location inter-dependence. 

While this definition borrows heavily from the Pufferfish framework, it has a few key modifications. Pufferfish is generally described from a central, not local model. We specialize the kinds of secrets and discriminative pairs for the case of local location trace privacy. Additionally, we specialize the type of prior distribution class needed for this local setting: the conditional prior class. Finally, we relax the strict max divergence (max log odds) criterion of the Pufferfish definition to a R\'enyi divergence. This guarantees that --- with high probability on draws of \emph{realistic} traces $Z|\Xs$ --- the log odds will be bounded by $\varepsilon$. As $\lambda \rightarrow \infty$, the log odds are bounded for all traces, i.e. the max divergence is bounded. We formalize this in Theorem \ref{thm: prior-posterior}. 

The R\'enyi criterion of CIP greatly improves its flexibility. Unlike the standard DP Approaches A$\rightarrow$C which only take probabilities over the mechanism, we do not have full control over the randomness at play: it is partially from $\calA$ defined by us and from $\calP$ intrinsic to the data. Unlike max divergence, R\'enyi divergence is available in closed form for many distributions, allowing for a more flexible privacy framework. The $\lambda$ parameter helps us tune how strict a CIP definition is and how much noise we need to add. This allows us to design mechanisms that are resistant to natural models of dependence while preserving utility. 

%Second, providing a reasonable bound on the max divergence -- even for a restricted conditional prior class -- would require adding an unreasonable amount of noise and destroy utility.

\subsection{Properties}

We now identify key properties that make the CIP guarantee interpretable and robust. 

\paragraph{Interpretability:} CIP guarantees that a Bayesian adversary with any prior distribution on traces $\calP$ in the conditional prior class $\Theta$ does not learn much about basic or compound secrets from the released trace $Z$. For basic secrets, this means that the adversary's posterior beliefs regarding sensitive location $\Xs$ are not much sharper than their prior beliefs before witnessing $Z$.  
\begin{theorem} \emph{Prior-Posterior Gap:} 
\label{thm: prior-posterior}
	An $(\varepsilon, \lambda)$-CIP mechanism with conditional prior class $\Theta$ guarantees that for any event $O$ on sanitized trace $Z$
	\begin{align*}
		\bigg| \log \frac{P_{\calP, \calA}(s_i | Z \in O)}{P_{\calP, \calA}(s_j | Z \in O)} - \log \frac{P_{\calP}(s_i)}{P_{\calP}(s_j)} \bigg| \leq \varepsilon'
	\end{align*}
	for any $\calP \in \Theta$ with probability $\geq 1 - \delta$ over draws of $Z|\Xs=s_i$ or $Z|\Xs=s_j$, where $\varepsilon'$ and $\delta$ are related by
	\begin{align*}
		\varepsilon' = \varepsilon + \frac{\log \nicefrac{1}{\delta}}{\lambda - 1} \ .
	\end{align*}
	This holds under the condition that $Z|\Xs = s_i$ and $Z|\Xs = s_j$ have identical support. 
\end{theorem}
A CIP mechanism depends only on the conditional prior describing the data, not the data itself. Suppose an adversary's prior beliefs on $\Xs$ are uniform over some region. For $\lambda = 5$ and $\varepsilon = 0.1$, there is only a $\approx 1\%$ chance that their posterior odds on $s_i,s_j$ will be more than 3.5, and a $\approx 10\%$ chance that they will be more than 2. This `chance' is over draws of likely remaining locations $\Xu$ and the additive noise $G$.  Proofs of all results are in Appendix \ref{apx: proofs}.

For additive noise mechanisms like $\calA(X) = X + G = Z$, the CIP loss can be split into two terms: one accounting for the direct privacy loss of $\Zs$ on $\Xs$ and a second accounting for the inferential privacy loss of $\Zu$ on $\Xs$ via $\Xu$.

\begin{lemma}\emph{Conditional Independence}
	\label{lem: renyi additive loss}
	For an additive noise mechanism, a fully dependent trace as in \textbf{Figure \ref{fig:full model}}, and any prior $\calP$ on $X$ the CIP loss may be expressed as
	\begin{align}
	\label{eqn:two terms}
		&D_\lambda \binom{P_{\calA, \calP}(Z | \Xs = s_i)}{P_{\calA, \calP}(Z | \Xs = s_j)}  \\ 
		\vspace{2em}
		&= \sum_{i \in \Is} \bigg[ D_\lambda \binom{P_\calA(Z_i | X_i = s_i)}{P_\calA(Z_i | X_i = s_j)} \bigg]
		+ D_\lambda \binom{P_{\calA, \calP}(\Zu | \Xs = s_i)}{P_{\calA, \calP}(\Zu | \Xs = s_j)} \notag
	\end{align}
\end{lemma} 
One interpretation of GI is that it assumes all locations $X_i$ are independent. In this case, the second term vanishes and the privacy loss only depends on randomness of the mechanism, not the prior. 

\paragraph{Robustness:}
\cite{no_free_lunch} show that it is impossible to achieve both utility and privacy resistant to all priors. CIP provides resistance to a reasonable class of priors $\calP \in \Theta$, but it is possible that the true distribution $\calQ \notin \Theta$. In this case, the privacy guarantees degrade gracefully as the divergence between $\calQ$ and $\calP \in \Theta$ grows. 
\begin{theorem}\emph{Robustness to Prior Misspecification}
\label{thm: prior misspecification}
	Mechanism $\calA$ satisfies $\varepsilon(\lambda)$-CIP for prior class $\Theta$. Suppose the finite mean true distribution $\calQ$ is not in $\Theta$. The CIP loss of $\calA$ against prior $\calQ$ is bounded by 
	\begin{align*}
		D_\lambda \binom{P_{\calA, \calQ}(Z | \Xs = s_i)}{P_{\calA, \calQ}(Z | \Xs = s_j)} \leq \varepsilon'(\lambda)
	\end{align*}
	where
	\begin{align*}
		\varepsilon'(\lambda) 
		&= \frac{\lambda - \frac{1}{2}}{\lambda - 1} \ \Delta(2\lambda) + 
		\Delta(4\lambda - 3) +
		\frac{2\lambda - \frac{3}{2}}{2\lambda - 2} \ \varepsilon(4 \lambda -2)
	\end{align*}
	and where $\Delta(\lambda)$ is
	\begin{align*}
		\inf_{\calP \in \Theta} \sup_{s_i \in \calS} \max \bigg\{ 
		D_\lambda \binom{P_{ \calP}(\Xu | \Xs = s_i)}{P_{ \calQ}(\Xu | \Xs = s_i)}, 
		D_\lambda \binom{P_{ \calQ}(\Xu | \Xs = s_i)}{P_{ \calP}(\Xu | \Xs = s_i)}
		\bigg\}
	\end{align*}
\end{theorem}

As long as the conditional distribution on $\Xu|\Xs = s_i$ of prior $\calQ$ is close to that of some $\calP \in \Theta$, the privacy guarantees should change only marginally. This bound is tightest when $\varepsilon(\lambda)$ does not grow quickly with order $\lambda$.



%
%\begin{center}
%\begin{figure*}
%\begin{minipage}[t]{0.26\textwidth}
%	\begin{algorithm}[H]
%		\SetAlgoLined
%		\KwInput{$\Is, \Sigma, o_t$}
%		\KwOutput{$\Sigmag$}
%			\vskip 2mm
%			$\argmax_{\Sigmag \succeq 0} \  \beta^*$\;
%			\vskip 1mm
%			$ \text{s.t. } \tilde{A}^{-1} \tilde{B}^{-1} \tilde{A}^{-\intercal} \succeq \beta^* \mathbf{I}$\;
%			$ \quad \ \ \ \trace(\Sigmag) \leq n o_t$\;
%			\vskip 2mm
%		\Return $\Sigmag$\;
%		\caption{SIG OPT}
%	\label{alg: sig opt}
%	\end{algorithm}
%\end{minipage}%
%\hspace{0.04\textwidth}
%\begin{minipage}[t]{0.3\textwidth}
%	\begin{algorithm}[H]
%		\SetAlgoLined
%		\KwInput{$\calF$}
%		\KwOutput{$\Sigmag$}
%			\vskip 2mm
%			$\argmin_{\Sigmag } \  \trace(\Sigmag)$\;
%			\vskip 1mm
%			$ \text{s.t. } \Sigmag \succeq \Sigmag_i , \ \forall \Sigmag_i \in \calF$\;
%			\vskip 2mm
%		\Return $\Sigmag$\;
%		\caption{ALL SECRETS}
%	\label{alg: all secrets}
%	\end{algorithm}
%\end{minipage}
%\hspace{0.04\textwidth}
%\begin{minipage}[t]{0.34\textwidth}
%	\begin{algorithm}[H]
%		\SetAlgoLined
%		\KwInput{$\mathbb{I}_{\calS_b}, \Sigma, o_t$}
%		\KwOutput{$\Sigmag$}
%		\vskip 2mm
%		$\calF = \emptyset$\;
%		
%		\For{$\mathbb{I}_{S_i} \in \mathbb{I}_{\calS_b}$}
%			{
%				$\Sigmag_i =$ SIG OPT$(\mathbb{I}_{S_i}, \Sigma, o_t)$\; 
%				
%				$\calF = \calF \cup \Sigmag_i$\;
%			}
%		\vskip 2mm
%			\Return ALL SECRETS$(\calF)$\;
%	
%	\caption{Basic Mechanism}
%	\label{alg: basic mechanism}
%	\end{algorithm}
%\end{minipage}%	
%\end{figure*}
%\end{center}
%

\subsection{CIP for Gaussian Process Priors}
\label{sec: CIP for GP} 
A \emph{GP conditional prior class} is the set of all GP prior distributions with the same kernel function $(i,j) \rightarrow \text{Cov}(X_i, X_j)$ and any mean function $i \rightarrow \E[X_i]$. With an additive Gaussian mechanism $G \sim \calN(\mathbf{0}, \Sigmag)$, the CIP loss of Equation \eqref{eqn:two terms} can be bounded for any GP conditional prior class. See Appendix \ref{apx: GP prior class} for further discussion of the GP conditional prior class. 

\begin{theorem}\emph{CIP loss bound for GP conditional priors:}
\label{thm:GP bound}
	Let $\Theta$ be a GP conditional prior class. Let $\Sigma$ be the covariance matrix for $X$ produced by its kernel function. Let $\calS$ be the basic or compound secret associated with $\Is$, and $S$ be the number of unique times in $\Is$. The mechanism $\calA(X) = X + G = Z$, where $G \sim \calN(\mathbf{0}, \Sigmag)$, then satisfies $(\varepsilon, \lambda)$-Conditional Inferential Privacy $(\Spairs, r, \Theta)$, where 
	\begin{align}
		\varepsilon
		&\leq \frac{\lambda}{2} S r^2 \Big(  \frac{1 }{\sigma_s^2} + \alpha^*  \Big) 
		\label{eqn: priv bound}
	\end{align}
	\text{ }\vspace{1mm}\\
	where $\sigma_s^2$ is the variance of each $G_i \in \Gs$ (diagonal entries of $\Sigmag_{ss}$) and $\alpha^*$ is the maximum eigenvalue of $\Sigmaeff = \big(\Sigma_{us} \Sigma_{ss}^{-1}\big)^\intercal \big( \Sigma_{u | s} + \Sigma_{uu}^{(g)} \big)^{-1} \big(\Sigma_{us} \Sigma_{ss}^{-1}\big)$. 
\end{theorem}

The above bound is tight for basic secrets ($S = 1$). The two terms of Equation \eqref{eqn: priv bound} represent the direct $(\frac{1}{\sigma_s^2})$ and inferential $(\alpha^*)$ loss terms of Equation \eqref{eqn:two terms}. We assume that each diagonal entry of $\Sigmag_{ss}$ equals some $\sigma_s^2$, so that each $X_i \in \Xs$ experiences identical direct privacy loss, which is optimal under utility constraints. 

The above bound composes gracefully when multiple traces of an individual are released. 

\begin{corollary}\emph{Graceful Composition in Time}
\label{cor: composition}
	Suppose a user releases two traces $X$ and $\hat{X}$ with additive noise $G \sim \calN(\mathbf{0}, \Sigmag)$ and $\hat{G} \sim \calN(\mathbf{0}, \hat{\Sigma}^{(g)})$, respectively. Then basic or compound secret $\Xs$ of $X$ enjoys $(\bar{\varepsilon}, \lambda)$-CIP, where 
	\begin{align*}
		\bar{\varepsilon} \leq \frac{\lambda}{2} S r^2 \Big(  \frac{1 }{\sigma_s^2} + \bar{\alpha}^*  \Big) 
	\end{align*}
	and where $\bar{\alpha}^*$ is the maximum eigenvalue of $\bar{\Sigma}_{\text{eff}} = \big(\Sigma_{us} \Sigma_{ss}^{-1}\big)^\intercal \big( \Sigma_{u | s} + \bar{\Sigma}_{uu}^{(g)} \big)^{-1} \big(\Sigma_{us} \Sigma_{ss}^{-1}\big)$. $\Sigma$ is the covariance matrix of the joint distribution on $X, \hat{X}$ and 
	\begin{align*}
	\bar{\Sigma}^{(g)} =
		\begin{bmatrix}
			 \Sigmag & 0 \\
			 0 &  \hat{\Sigma}^{(g)} \ .
		\end{bmatrix}
	\end{align*}
\end{corollary}
\text{ } \vspace{2mm} \\
This bound is identical to that of Theorem \ref{thm:GP bound}, only using the joint distribution over $X$, $\hat{X}$ and $G, \hat{G}$. This provides some insight to the fact that, unlike DP, even parallel composition guarantees are not automatic. Composition depends on the conditional prior. In the GP setting, if the chosen kernel function decays over time, we can expect composition to have minimal effects on privacy for traces separated by long durations. 

To reduce the upper bound of Theorem \ref{thm:GP bound}, we optimize the correlation (off-diagonal) of $\Sigmag$ to minimize $\alpha^*$, and optimize its variance (diagonal) to balance a noise budget between lowering inferential ($\alpha^*$) and direct ($\frac{1}{\sigma_s^2}$) loss.