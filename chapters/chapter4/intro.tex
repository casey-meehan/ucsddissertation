\section{Introduction}
\label{sec:intro}
Differential Privacy (\textsf{DP}) and its local variant (\textsf{LDP}) are the most commonly accepted notions of data privacy. \ldp has the significant advantage of not requiring a trusted centralized aggregator, and has become a popular model for commercial deployments, such as those of Microsoft~\citep{Microsoft}, Apple~\citep{Apple}, and Google~\citep{Rappor1,Rappor2,Prochlo}. Its formal guarantee asserts that an adversary cannot infer the value of an individual's private input by observing the noisy output. However in practice, a vast amount of \textit{public auxiliary information}, such as address, social media connections, %(in terms of friends/followers), 
court records, property records, %\citep{home},
income and birth dates \citep{birth}, is available for every individual. An adversary, with access to such auxiliary information, \emph{can} learn about an individual's private data from several \emph{other} participants' noisy responses. We illustrate this as follows.%\vspace{-0.1cm}Its formal guarantee asserts that an adversary does not learn much about a participant's private input after observing the mechanism's noisy representation of it. However, an adversary \emph{can} draw reliable inferences about a participant's private input after observing several \emph{other} participants' noisy responses. Practically speaking, this is due to the fact that \ldp does not formally hide the identity of data owners: e.g. an \ldp response from a participant's browser can still be linked to that participant's device or account and thereby to their identity. With the preponderance of public auxiliary information available today (e.g. address, social media connections, court records, property records \nocite{home}, income and birth rates \citep{birth}), an adversary can track down other participants who (positively or negatively) correlate with them and leverage those responses to make a reliable inference of their underlying private input. We illustrate this as follows: 

% However in practice, a vast amount of \textit{public auxiliary information}, such as address, social media connections, %(in terms of friends/followers), 
% court records, property records \cite{home}, income and birth dates \citep{birth}, is available for every individual. An adversary with access to such auxiliary information \emph{can} learn about an individual's private data from several \emph{other} participants' noisy responses. We illustrate this as follows.%\vspace{-0.1cm}

\begin{tcolorbox}\vspace{-0.25cm}
\textbf{Problem.} An analyst runs a medical survey in Alice's community to investigate how the prevalence of a highly contagious disease changes from neighborhood to neighborhood. Community members report a binary value indicating whether they have the disease.  \vspace{-0.2cm}
\end{tcolorbox}\vspace{-0.1cm}
Next, consider the following two data reporting strategies. \vspace{-0.1cm}
\begin{tcolorbox}\vspace{-0.2cm} \textbf{Strategy $\mathbf{1}$.} Each data owner passes their data through an appropriate randomizer (that flips the input bit with some probability) in their local devices and reports the noisy output to the untrusted data analyst. 
\vspace{-0.2cm}
\end{tcolorbox}\vspace{-0.1cm}
\begin{tcolorbox}\vspace{-0.2cm} \textbf{Strategy $\mathbf{2}$.}  The noisy responses from the local devices of each of the data owners %\footnote{The individual  responses are appropriately anonymized by removing all identifiable information such as IP address of packets.} 
are collected by an intermediary trusted shuffler which dissociates the device IDs (metadata) from the responses and  uniformly randomly shuffles them before sending them to the analyst.\vspace{-0.2cm}
\end{tcolorbox}\vspace{-0.1cm}
\textbf{Strategy $\mathbf{1}$} corresponds to the standard \ldp deployment model (for example, Apple and Microsoft's deployments). Here \textit{the order of the noisy responses is informative of the identity of the data owners} -- the noisy response at index $1$ corresponds to the first data owner and so on. Thus, the noisy responses can be directly linked with its associated device/account ID and subsequently, auxiliary information. This puts Alice's data under the threat of inference attacks.
For instance, an adversary\footnote{The analyst and the adversary could be same, we refer to them separately for the ease of understanding.} may know the home addresses of the participants and use this to identify the responses of all the individuals from Alice's household.  
Being highly infectious, all or most of them 
 %\vspace{-0.4cm} 
\begin{figure*}[t]
\begin{minipage}{0.2\linewidth}
\begin{subfigure}[b]{\linewidth}
\centering
    \includegraphics[width=0.9\columnwidth]{./figures/data_radii.png}
        \caption{Original Data}
        \label{fig:data}
    \end{subfigure}
    \end{minipage}
    \begin{minipage}{0.8\linewidth}
    \begin{subfigure}[b]{0.25\linewidth}\centering
    \includegraphics[width=0.67\columnwidth]{./figures/LDP.png}\vspace{-0.25cm}
        \caption{\ldp}
        \label{fig:ldp}
    \end{subfigure}%%
    \begin{subfigure}[b]{0.25\linewidth}\centering
    \includegraphics[width=0.67\columnwidth]{./figures/Shuffle_r1.png}\vspace{-0.25cm}
        \caption{Our scheme: $r_a$}
        \label{fig:r}\end{subfigure}%%
    \begin{subfigure}[b]{0.25\linewidth}\centering
    \includegraphics[width=0.67\columnwidth]{./figures/shuffle_r2.png}\vspace{-0.25cm}
        \caption{Our scheme: $r_b$}
        \label{fig:R}\end{subfigure}%%
      \begin{subfigure}[b]{0.25\linewidth}\centering
    \includegraphics[width=0.67\columnwidth]{./figures/Uniform.png}\vspace{-0.25cm}
        \caption{Uniform shuffle}
        \label{fig:uniform}
    \end{subfigure}\\ \vspace{-0.05cm}
    \begin{subfigure}[b]{0.25\linewidth}\centering
    \includegraphics[width=0.67\columnwidth]{./figures/LDP_data_1.png}\vspace{-0.25cm}
        \caption{Attack: LDP}
        \label{fig:ldp:attack}
    \end{subfigure}%%
    \begin{subfigure}[b]{0.25\linewidth}\centering
  \includegraphics[width=0.67\columnwidth]{./figures/r1_attack.png}\vspace{-0.25cm}
       \caption{Attack: $r_a$}
        \label{fig:r:attack}\end{subfigure}%%
    \begin{subfigure}[b]{0.25\linewidth}\centering
   \includegraphics[width=0.67\columnwidth]{./figures/r2_attack.png}\vspace{-0.25cm}
        \caption{Attack: $r_b$}
        \label{fig:R:attack}\end{subfigure}%%
      \begin{subfigure}[b]{0.25\linewidth}\centering
   \includegraphics[width=0.67\columnwidth]{./figures/attack_uniform.png}\vspace{-0.25cm}
        \caption{Attack: unif. shuff.}
        \label{fig:uniform:attack}
    \end{subfigure}
 \end{minipage}\vspace{-0.15cm}
    \caption[Demonstration of how our proposed scheme thwarts inference attacks at different granularities.]{Demonstration of how our proposed scheme thwarts inference attacks at different granularities. Fig. \ref{fig:data}  depicts the original sensitive data (such as income bracket) with eight color-coded labels. The position of the points represents public information (such as home address) used to correlate them. There are three levels of granularity: warm vs. cool clusters, blue vs. green and red vs. orange crescents, and light vs. dark within each crescent. Fig. \ref{fig:ldp} depicts \scalebox{0.9}{$\epsilon = 2.55$} \textsf{LDP}. Fig. \ref{fig:r} and \ref{fig:R} correspond to our scheme, each with $\alpha = 1$ (privacy parameter, Def. \ref{def:privacy}). The former uses a smaller  distance threshold ($r_1$, used to delineate the granularity of grouping -- see Sec. \ref{sec:privacy:def}) that mostly shuffles in each crescent. The latter uses a larger distance threshold ($r_2$) that shuffles within each cluster. Figures in the bottom row  demonstrate an inference attack (uses Gaussian process correlation) %with a fixed length scale) 
    on all four cases. We see that  \ldp  reveals almost the entire dataset (Fig. \ref{fig:ldp:attack}) while uniform shuffling prevents all classification (\ref{fig:uniform:attack}). However, the granularity can be controlled with our scheme (Figs. \ref{fig:r:attack}, \ref{fig:R:attack}). }
   \label{fig:demonstration}
%   \vspace{-0.15cm}
    \end{figure*}
  will have the same true value ($0$ or $1$). Hence, the adversary can reliably infer Alice's value by taking a simple majority vote of her and her household's noisy responses. Note that this does not violate the \ldp guarantee since the inputs are appropriately randomized when observed in isolation. Additionally, on account of being public, the auxiliary information is known to the adversary (and analyst) \emph{a priori} -- no mechanism can prevent their disclosure. For instance, any attempts to include Alice's address as an additional feature of the data and then report via \ldp is \emph{futile} --   the adversary would simply discard the reported noisy address and use the auxiliary information about the exact addresses to identify the responses of her household members. We call such threats \emph{inference attacks} -- recovering an individual's private input using all or a subset of other participants' noisy responses. It is well known that protecting against inference attacks that rely on underlying data correlations is beyond the purview of \DP  \citep{Pufferfish, sok}. 
  
\textbf{Strategy 2} corresponds to the recently introduced shuffle \DP model, such as Google's Prochlo \citep{Prochlo}.  Here, the noisy responses are completely anonymized -- the adversary cannot identify which \ldp responses correspond to Alice and her household.  Under such a model, only information that is completely order agnostic (i.e., symmetric functions that can be computed over just the \textit{bag} of values, such as aggregate statistics) can be extracted. Consequently, the analyst also fails to accomplish their original goal as all the underlying data correlation is destroyed.
 
Thus, we see that the two models of deployment for \ldp present a trade-off between vulnerability to inference attacks and scope of data learnability. In fact, as demonstrated in \cite{Kifer}, it is impossible to defend against \emph{all} inference attacks while simultaneously maintaining utility for learning. In the extreme case that the adversary knows \emph{everyone} in Alice's community has the same true value (but not which one), no mechanism can prevent revelation of Alice's datapoint short of destroying all utility of the dataset. This then begs the question: \textbf{\emph{Can we formally suppress \underline{specific} inference attacks targeting each data owner while maintaining some meaningful learnability of the private data?}} Referring back to our example, can we thwart attacks inferring Alice's data using specifically her households' responses and still allow the medical analyst to learn its target trends? Can we offer this to every data owner participating?
 
%In this paper, we strike a balance and we propose a generalized shuffle framework for deployment that can interpolate between the two extremes.
In this paper, we strike a balance and propose a generalized shuffle framework that meets the utility requirements of the above analyst while formally protecting data owners against inference attacks. 
Our solution is based on the key insight: \textit{the order of the data acts as the proxy for the identity of data owners} as illustrated above. The granularity at which the ordering is maintained formalizes resistance to inference attacks while retaining some meaningful learnability of the private data. Specifically, we guarantee each data owner that their data is shuffled together with a carefully chosen group of other data owners. Revisiting our example, consider uniformly shuffling the responses from Alice's household and her immediate neighbors. Now an adversary cannot use her household's responses to predict her value any better than they could with a random sample of responses from this group. 
In the same way that \ldp prevents reconstruction of her datapoint using specifically \emph{her} noisy response, this scheme prevents reconstruction of her datapoint using specifically \emph{her households'} responses. The real challenge is offering such guarantees \textit{equally} to \textit{every} data owner. Bob, Alice's neighbor, needs his households' responses shuffled in with his neighbors, as does Luis who is a neighbor of Bob but \textit{not} of Alice. Thus, we have $n$ data owners with $n$ distinct, overlapping groups. Our scheme supports arbitrary groupings (overlapping or not), introducing a diverse and tunable class of privacy/utility trade-offs which is not attainable with either \ldp or uniform shuffling alone.
%This disallows the trivial strategy of shuffling the noisy responses of each group uniformly. To this end, we propose shuffling the responses in a systematic manner that tunes the privacy guarantee, trading it off with data learnability. 
For the above example, our scheme can formally protect each data owner from inference attacks using specifically their household, while still learning how disease prevalence changes across the neighborhoods of Alice's community.\\
This work offers two key contributions to the machine learning privacy literature: 
 \vspace{-0.2cm}
\squishlistfour
    \item \textbf{Novel privacy guarantee.} We propose a novel privacy definition, \name-privacy that captures the privacy of the \textit{order} of a data sequence (Sec. \ref{sec:privacy:def}) and formalizes the degree of resistance against inference attacks (Sec. \ref{sec:privacy:implications}). \name-privacy allows assigning an arbitrary group, $G_i$, to each data owner, $\DO_i, i \in [n]$. For instance, the groups can represent individuals in the same age bracket, `friends' on social media, or individuals living in each other's vicinity (as in case of Alice in our example).  Recall that the order is informative of the data owner's identity. Intuitively,  \name-privacy protects $\DO_i$ from inference attacks that arise from knowing the \textit{identity} of the members of their group $G_i$ (Sec. \ref{sec:privacy:implications}). %\textcolor{blue}{By shuffling within each group, \name-privacy protects $\DO_i$ against inference attacks that utilize the data of any subset of the members of $G_i$, while still revealing the statistics of each group to the analyst.} 
    %The group assignment is based on a public auxiliary information  -- individuals of a single group are `similar' w.r.t the auxiliary information. 
    Additionally, this grouping determines a threshold of learnability -- any learning that is order agnostic within a group (disease prevalence in a neighborhood -- the data analyst's goal in our example) is utilitarian and allowed; whereas analysis that involves identifying the values of individuals within a group (disease prevalence within specific households -- the adversary's goal) is regarded as a privacy threat and protected against.
 See Fig. \ref{fig:demonstration} for a toy demonstration of how our guarantee allows \textit{tuning the granularity at which trends can be learned}. 
    
    \item \textbf{Novel shuffle framework.} We  propose a novel mechanism that shuffles the data systematically and achieves \name-privacy. This 
   provides a generalized shuffle framework that interpolates between no shuffling (\textsf{LDP}) and uniform random shuffling (shuffle model) in terms of protection against inference attacks and data learnability.
    %\textcolor{blue}{allows analysis of subsets of the reported data (such as within neighborhoods across a city) while 
    %Our experimental results (Sec. \ref{sec:eval}) demonstrates its efficacy against realistic inference attacks.  
\squishendfour 
  \vspace{-0.4cm} 
%   \arc{TO-DOs \\1)Include discussion about viewing a sequence as <bag of val, order> 2) discussion about side information - why is it immutable 3) highlight privacy guarantee - users choose groups which should be overlapping for generality  }
\section{Related Work}\label{sec:related_work} \vspace{-0.3cm} 
The shuffle model of \DP \citep{Bittau2017,shuffle2,shuffling1} differs from our scheme as follows. These works $(1)$ study \DP benefits of shuffling whereas we study the inferential privacy benefits, and $(2)$ only study uniformly random shuffling where ours generalizes this to tunable, non-uniform shuffling (see App. \ref{app:related}). 
% Where these works study the differential private benefits of shuffling (lower $\epsilon$), our work studies its inferential privacy benefits. Furthermore, those works only consider uniform random shuffling of the dataset, where our approach  allows configurable, non-uniform shuffling distributions that allow us to tune the tradeoff between inferential risk and the granularity of learnable trends.
% These works differ from our approach in two ways. First, they only study shuffling as a means to amplify local differentially private guarantees, effectively offering a lower $\epsilon$ when viewed from the alternative, central \DP model. Our results cater to local \emph{inferential} privacy (Sec. \ref{sec:ip}), limiting what can be inferred about one individual from other individuals' responses. Second, the shuffle model only studies uniform random shuffling of the dataset. In contrast, our approach allows configurable, non-uniform shuffling distributions that allow us to tune the tradeoff between inferential risk and the granularity of learnable trends. 
\\A steady line of work has studied inferential privacy  \citep{semantics, Kifer,  IP, Dalenius:1977, dwork2010on, sok}. %Kifer et al. \cite{Kifer} formally studied privacy degradation in the face of data correlations and later proposed a  privacy framework, Pufferfish \cite{Pufferfish, Song,Blowfish}, for analyzing inferential privacy. 
%Subsequently, several other privacy definitions have also been proposed for the inferential privacy setting \cite{DDP,BDP,correlated,correlated2,CWP}. 
Our work departs from those in that we focus on \emph{local} inferential privacy and do so via the new angle of shuffling. 
\\Older works such as $k$-anonymity \citep{kanon},  $l$-diversity \cite{ldiv}, Anatomy \citep{anatomy} and others \citep{older1, older2, older3, older4, older5} have studied the privacy risk of non-sensitive auxiliary information or `quasi identifiers'. These works $(1)$ focus on the setting of dataset release, whereas we focus on dataset collection, and $(2)$ do not offer each data owner formal inferential guarantees, whereas we do. %As such, QIs can be manipulated and controlled, whereas we place no restriction on the amount or type of auxiliary information accessible to the adversary, nor do we control it. 
%Additionally, our work offers each individual formal inferential guarantees against informed adversaries, whereas those works do not. 
The De Finetti attack \citep{definetti} shows how shuffling schemes are vulnerable to inference attacks that correlate records to recover the original permutation of sensitive attributes. A strict instance of our privacy guarantee can thwart such attacks (at the cost of no utility, App. \ref{app:de finetti}). 