\begin{figure*}[ht]
\centering 
   \begin{subfigure}[b]{0.30\linewidth}
       \centering
       \includegraphics[width=0.95\linewidth]{./figures/20news_sweep_eps.png}
      \vspace{-0.15cm}
       \caption{\textit{20 News}: Sweep $\epsilon$}
       \label{fig:eps:tnews}
    \end{subfigure}%%
    \begin{subfigure}[b]{0.30\linewidth}
       \centering
       \includegraphics[width=0.95\linewidth]{./figures/gr_sweep_eps.png}
      \vspace{-0.15cm}
       \caption{\textit{GoodReads}: Sweep $\epsilon$}
       \label{fig:eps:gr}
    \end{subfigure}%%
    \begin{subfigure}[b]{0.30\linewidth}
       \centering
       \includegraphics[width=0.95\linewidth]{./figures/imdb_sweep_eps.png}
      \vspace{-0.15cm}
       \caption{\textit{IMDB}: Sweep $\epsilon$}
       \label{fig:eps:imdb}
    \end{subfigure}%%
    \hfill
    \begin{subfigure}[b]{0.30\linewidth}
       \centering
       \includegraphics[width=0.95\linewidth]{./figures/20news_sweep_stcs.png}
      \vspace{-0.15cm}
       \caption{\textit{20 News}: Sweep $k$}
       \label{fig:k:tnews}
    \end{subfigure}%%
    \begin{subfigure}[b]{0.30\linewidth}
       \centering
       \includegraphics[width=0.95\linewidth]{./figures/gr_sweep_stcs.png}
      \vspace{-0.15cm}
       \caption{\textit{GoodReads}: Sweep $k$}
       \label{fig:k:gr}
    \end{subfigure}%%
    \begin{subfigure}[b]{0.30\linewidth}
       \centering
       \includegraphics[width=0.95\linewidth]{./figures/imdb_sweep_stcs.png}
      \vspace{-0.15cm}
       \caption{\textit{IMDB}: Sweep $k$}
       \label{fig:k:imdb}
    \end{subfigure}%%
    \caption{Comparison of our mechanism with two baselines: truncation \cite{clifton} and word-level Metric DP \cite{metricdp} for both sentiment analysis (\emph{IMDB}) and topic classification (\emph{GoodReads}, \emph{20News}) on private, unsupervised embeddings. All plots show test-set macro $F_1$ scores. The top row shows performance vs. privacy parameter $\epsilon$ (lower is better privacy). The bottom row shows performance vs. number of sentences $k$ with $\epsilon = 10$. \technique\ outperforms both baselines across datasets and tasks. Note that at a given $\epsilon$, word-level Metric-DP is a significantly weaker privacy guarantee.}
\end{figure*}
