\section{Introduction} 
\label{sec:intro} 



Language models have now become ubiquitous in NLP \cite{devlin2019bert, liu2019roberta, alsentzer2019publicly}, pushing the state of the art in a variety of tasks \cite{strubell2018linguistically, liu2019multi, mrini-etal-2021-recursive}. While language models capture meaning and various linguistic properties of text \cite{jawahar2019does, yenicelik2020does}, an individual's written text can include highly sensitive information. Even if such details are not needed or used, sensitive information has been found to be vulnerable and detectable to attacks \cite{pan2020privacy, attack_word_embs, carlini_attack}. Reconstruction attacks \cite{xie2021reconstruction} have even successfully broken through private learning schemes that rely on encryption-type methods \cite{huang-etal-2020-texthide}.

As of now, there is no broad agreement on what constitutes good privacy for natural language \cite{kairouz2019advances}. \citet{huang-etal-2020-texthide} argue that different applications and models require different privacy definitions. Several emerging works propose to apply Metric Differential Privacy \cite{orig_metricdp} at the word level \cite{metricdp,  mdp_low_dim, TEM, another_metric_DP, fancy_metricdp, metricDP_gumbel} . They propose to add noise to word embeddings, such that they are indistinguishable from their nearest neighbours.

At the document level, however, the above definition has two areas for improvement. First, it may not offer the level of privacy desired. Having each word indistinguishable with similar words may not hide higher level concepts in the document, and may not be satisfactory for many users. Second, it may not be very interpretable or easy to communicate to end-users, since the privacy definition relies fundamentally on the choice of embedding model to determine which words are indistinguishable with a given word. This may not be clear and precise enough for end-users to grasp.
%
%The above definition is straightforward to implement and naturally takes advantage of the structure in precomputed word embeddings. At the document level, however, there are two areas for improvement. First, it may not offer the level of privacy desired. Having each word indistinguishable only with similar words may not be satisfactory for many users. Replacing words with similar words does not necessarily hide higher level concepts in the document. Second, it may not be very interpretable or easy to communicate to end-users. The privacy definition relies fundamentally on the choice of embedding model. Stating which words are indistinguishable with a given word requires querying the embedding model. This may not be a clear and precise enough for end users to grasp.

\begin{figure}
	\centering
	\includegraphics[width = 0.8\linewidth]{figures/first_pg.png}
	\label{fig:first page}
	\vspace{0cm}
	\caption{$x$ and $x'$ yield $z \in \R^d$ with similar probability.}
\end{figure}
 
\begin{figure*}
	\centering
	\vspace{-1cm}
	\includegraphics[width = \linewidth]{figures/block_diagram.png}
	\label{fig:block diagram}
	\vspace{-0.65cm}
	\caption[\technique\ generates a private embedding $z$ of document $x$ by selecting from a set $F$ of public, non-private document embeddings.]{\technique\ generates a private embedding \textcolor{green}{$z$} of document \textcolor{red}{$x$} by selecting from a set \textcolor{blue}{$F$} of public, non-private document embeddings. Sentences from \textcolor{red}{$x$} are encoded by $G'$. The privacy mechanism $\mname$, then privately samples from \textcolor{blue}{$F$}, with a preference for candidates with high Tukey Depth, `deep candidates'. $G'$ is trained beforehand to ensure that deep candidates are likely to exist and are relevant to \textcolor{red}{$x$}.}
\end{figure*}

In this work, we propose a new privacy definition for documents: sentence privacy. This guarantee is both strong and interpretable: any sentence in a document must be indistinguishable with \emph{any} other sentence. A document embedding is sentence-private if we can replace any single sentence in the document and have a similar probability of producing the same embedding. As such, the embedding only stores limited information unique to any given sentence. This definition is easy to communicate and strictly stronger than word-level definitions, as modifying a sentence can be changing one word.

Although this definition is strong, we are able to produce unsupervised, general embeddings of documents that are useful for downstream tasks like sentiment analysis and topic classification. To achieve this we propose a novel privacy mechanism, \technique, which privately samples a high-dimensional embedding from a preselected set of candidate embeddings derived from public, non-private data. \technique\  works by first pre-tuning a sentence encoder on public data such that semantically different document embeddings are far apart from each other. Then, we approximate each candidate's Tukey Depth within the private documents' sentence embeddings. Deeper candidates are the most likely to be sampled to represent the private document. We evaluate \technique\  on three illustrative datasets, and show that these unsupervised private embeddings are useful for both sentiment analysis and topic classification as compared to baselines. 

In summary, this work makes the following contributions to the language privacy literature:

\begin{squishlist}
	\item A new, strong, and interpretable privacy definition that offers complete indistinguishability to each sentence in a document. 
	\item A novel, unsupervised embedding technique, \technique, to generate sentence-private document embeddings. 
	\item An empirical assessment of \technique, demonstrating its advantage over baselines, delivering strong privacy and utility. 
\end{squishlist}