\section{Background and Related Work}

\paragraph{Setting.}We denote a `document' as a sequence of sentences. Let $s \in \calS$ be any finite-length sentence. Then, the space of all documents is $\calX = \calS^*$ and document $x \in \calX$ is written as $x = (s_1, s_2, \dots, s_k)$ for any non-negative integer $k$ of sentences. In this work, we focus on cohesive documents of sentences written together like reviews or emails, but our methods and guarantees apply to any sequence of sentences, such as a collection of messages written by an individual over some period of time.

Our task is to produce an embedding $z \in \R^d$ of any document $x \in \calX$ such that any single sentence $s_i \in x$ is indistinguishable with every other sentence $s_i' \in \calS \backslash s_i$. That is, if one were to replace any single sentence in the document $s_i \in x$ with \emph{any other} sentence $s_i' \in \calS \backslash s_i$, the probability of producing a given embedding $z$ is similar. To achieve this, we propose a randomized embedding function (the embedding \emph{mechanism}) $\calM : \calX \rightarrow \R^d$, that generates a private embedding $z = \calM(x)$ that is useful for downstream tasks. 

\subsection{Differential Privacy}
The above privacy notion is inspired by Differential Privacy (DP) \cite{DP}. It guarantees that --- whether an individual participates (dataset $D$) or not (dataset $D'$) --- the probability of any output only chances by a constant factor. 

\begin{definition}[Differential Privacy]
	Given any pair of datasets $D, D' \in \calD$ that differ only in the information of a single individual, we say that the mechanism $\calA : \calD \rightarrow \calO$, satisfies $\epsilon$-DP if 
	\begin{align*}
		\Pr[\calA(D) \in O] \leq e^\epsilon \Pr[\calA(D') \in O]
	\end{align*}
	for any event $O \subseteq \calO$. 
\end{definition}
Note that we take probability over the randomness of the mechanism $\calA$ only, not the data distribution. DP has several nice properties that make it easy to work with including closure under post-processing, an additive privacy budget (composition), and closure under group privacy guarantees (guarantees to a \emph{subset} of multiple participants). See \citealt{DPbook} for more details. 

%A useful property of DP is closure under post-processing: if an $\epsilon$-DP mechanism produces output $o$, then anything derived from $o$, $u = f(o)$, also satisfies $\epsilon$-DP. 
%
%\begin{corollary}[Post-processing]
%\label{cor:post processing}
%	If $\calA : \calD \rightarrow \calO$ satisfies $\epsilon$-DP, then so does $f \circ \calA$ for any (possibly randomized) function $f : \calO \rightarrow \calU$. 
%\end{corollary}
%
%Additionally, if we release $r$ queries from a given dataset, each of which satisfies $\epsilon$-DP, the \emph{composition} of these satisfies at least $r\epsilon$-DP. 
%
%\begin{corollary}[Composition]
%	If mechanisms $\calA_1, \calA_2, \dots, \calA_r$ each satisfy $\epsilon$-DP, then the composition of them $\calA(D) = \big(\calA_1(D), \calA_2(D), \dots, \calA_r(D)\big)$ satisfies $\epsilon'$-DP for some $\epsilon' \leq r\epsilon$. 
%\end{corollary}
%
%Finally, DP offers closure under \emph{group privacy}. If two datasets differ in $r \geq 1$ elements, they are still indistinguishable with $r \epsilon$-DP. 
%
%\begin{corollary}[Group privacy]
%\label{cor:group privacy} 
%	If $\calA : \calD \rightarrow \calO$ satisfies $\epsilon$-DP, then 
%	\begin{align*}
%		\Pr[\calA(D) \in O] \leq e^{r\epsilon} \Pr[\calA(D') \in O]
%	\end{align*}
%	for any pair of datasets $D, D' \in \calD$ that differ in $r$ elements. 
%\end{corollary} 
%
The \emph{exponential mechanism} \cite{exp_mech} allows us to make a DP selection from an arbitrary output space $\calO$ based on private dataset $D$. A \emph{utility function} over input/output pairs, $u : \calD \times \calO \rightarrow \R$ determines which outputs are the best selection given dataset $D$. The log probability of choosing output $o \in \calO$ when the input is dataset $D \in \calD$ is then proportional to its utility $u(D,o)$. The \emph{sensitivity} of $u(\cdot, \cdot)$ is the worst-case change in utility over pairs of neighboring datasets $(D,D')$ that change in one entry, $\Delta u = \max_{D, D', o} | u(D,o) - u(D', o)|$. 
\begin{definition}
\label{def: exp mech} 
	The \emph{exponential mechanism} $\calA_{Exp}: \calD \rightarrow \calO$ is a randomized algorithm with output distribution
	\vspace{-0.3cm}
	\begin{align*}
	\Pr[\calA_{Exp}(D) = o] \propto \exp\big( \frac{\epsilon u(D, o)}{2 \Delta u} \big) \quad .
	\end{align*}
\end{definition}

\subsection{Related Work}
\paragraph{Natural Language Privacy.} Previous work has demonstrated that NLP models and embeddings are vulnerable to reconstruction attacks \cite{carlini_attack, attack_word_embs, pan2020privacy}. In response there have been various efforts to design privacy-preserving techniques and definitions across NLP tasks. A line of work focuses on how to make NLP model training satisfy DP \cite{DP_training, DP_training_II}. This is distinct from our work in that it satisfies central DP -- where data is first aggregated non-privately and then privacy preserving algorithms (i.e. training) are run on that data. We model this work of the \emph{local} version of DP \cite{ldp}, wherein each individual's data is made private before centralizing. Our definition guarantees privacy to a single document as opposed to a single individual. 

A line of work more comparable to our approach makes documents locally private by generating a randomized version of a document that satisfies some formal privacy definition. As with the private embedding of our work, this generates locally private \emph{representation} of a given document $x$. The overwhelming majority of these methods satisfy an instance of Metric-DP \cite{orig_metricdp} at the word level \cite{metricdp,  mdp_low_dim, TEM, another_metric_DP, fancy_metricdp, metricDP_gumbel}. As discussed in the introduction, this guarantees that a document $x$ is indistinguishable with any other document $x'$ produced by swapping a single word in $x$ with a similar word. Two words are `similar' if they are close in the word embeddings space (e.g. GloVe). This guarantee is strictly weaker than our proposed definition, \SDP, which offers indistinguishability to any two documents that differ in an entire sentence. 

\paragraph{Privacy-preserving embeddings.} There is a large body of work on non-NLP privacy-preserving embeddings, as these embeddings have been shown to be vulnerable to attacks \cite{attack_on_embeddings}. \citet{clifton} attempt to generate locally private embeddings by bounding the embedding space, and we compare with this method in our experiments. \citet{kamath_high_dim} propose a method for privately publishing the average of embeddings, but their algorithm is not suited to operate on the small number of samples (sentences) a given document offers. Finally, \citet{private_halfspaces} propose a method for privately learning halfspaces in $\R^d$, which is relevant to private Tukey Medians, but their method would restrict input examples (sentence embeddings) to a finite discrete set in $\R^d$, a restriction we cannot tolerate. 


\section{Sentence-level Privacy}
We now introduce our simple, strong privacy definition, along with concepts we use to satisfy it. 
\subsection{Definition}
In this work, we adopt the \emph{local} notion of DP \cite{ldp}, wherein each individual's data is guaranteed privacy locally before being reported and centralized. Our mechanism $\calM$ receives a single document from a single individual, $x \in \calX$. We require that $\calM$ provides indistinguishability between documents $x, x'$ differing \emph{in one sentence}. 
%We call documents $x,x'$ \emph{neighboring documents}. 

\begin{definition}[Sentence Privacy, \SDP]
Given any pair of documents $x, x' \in \calX$ that differ only in one sentence, we say that a mechanism\\ $\calM : \calX \rightarrow \calO$ satisfies $\epsilon$-\SDP~if 
	\begin{align*}
		\Pr[\calM(x) \in O] \leq e^\epsilon \Pr[\calM(x') \in O]
	\end{align*}
	for any event $O \subseteq \calO$. 
\end{definition}

We focus on producing an embedding of the given document $x$, thus the output space is $\calO = \R^d$. For instance, consider the neighboring documents $x = (s_1, s_2, \dots, s_k)$ and $x' = (s_1, s_2', \dots, s_k)$ that differ in the second sentence, i.e. $s_2, s_2'$ can be \emph{any} pair of sentences in $\calS^2$. 
%If our embedding mechanism $\calM$ satisfies $\epsilon$-SentDP, we are guaranteed that 
%\begin{align*}
%	\log \bigg| \frac{\Pr[\calM(x) = z]}{\Pr[\calM(x') = z]} \bigg| \leq \epsilon 
%\end{align*}
%for any released embedding $z \in \R^d$. Thus for low values of $\epsilon$, no one sentence in the document can be reliably reconstructed from the embedding $z$ alone. 
This is a strong notion of privacy in comparison to existing definitions across NLP tasks. However, we show that we can guarantee SentDP while still providing embeddings that are useful for downstream tasks like sentiment analysis and classification. In theory, a SentDP private embedding $z$ should be able to encode any information from the document that is not unique to a small subset of sentences. For instance, $z$ can reliably encode the sentiment of $x$ as long as \emph{multiple} sentences reflect the sentiment. By the group privacy property of DP, which \SDP~maintains, two documents differing in $a$ sentences are $a\epsilon$ indistinguishable. So, if more sentences reflect the sentiment, the more $\calM$ can encode this into $z$ without compromising on privacy. 

\subsection{Sentence Mean Embeddings} 

Our approach is to produce a private version of the average of general-purpose sentence embeddings. By the post-processing property of DP, this embedding can be used repeatedly in any fashion desired without degrading the privacy guarantee. Our method makes use of existing pre-trained sentence encoding models. We denote this general sentence encoder as $G : \calS \rightarrow \R^d$. We show in our experiments that the mean of sentence embeddings,  
\begin{align}
	\overline{g}(x) = \frac{1}{k} \sum_{s_i \in x} G(s_i) \ , 
	\label{eqn: doc emb}
\end{align}
maintains significant information unique to the document and is useful for downstream tasks like classification and sentiment analysis.

We call $\overline{g}(x)$ the \emph{document embedding} since it summarizes the information in document $x$. While there exist other definitions of document embeddings \cite{yang2016hierarchical, thongtan2019sentiment, bianchi2020pre}, we decide to use averaging as it is a simple and established embedding technique \cite{bojanowski2017enriching, gupta2019better, li2020sentence}.
\subsection{Tukey Depth}
Depth is a concept in robust statistics used to describe how central a point is to a distribution. We borrow the definition proposed by \citet{tukeydepth}:

\begin{definition}
\label{def: tukey} 
	Given a distribution $P$ over $\R^d$, the Tukey Depth of a point $y \in \R^d$ is 
\begin{align*}
	\text{TD}_P(y) 
	&= \inf_{w \in \R^d} P\{y' : w \cdot (y' - y) \geq 0\} \quad. 
\end{align*} 
\end{definition}
In other words, take the hyperplane orthogonal to vector $w$, $h_w$, that passes through point $y$. Let $P_1^w$ be the probability under $P$ that a point lands on one side of $h_w$ and let $P_2^w$ be the probability that a point lands on the other side, so $P_1^w + P_2^w = 1$. $y$ is considered deep if $\min(P_1^w, P_2^w)$ is close to a half for \emph{all} vectors $w$ (and thus all $h$ passing through $y$). The \emph{Tukey Median} of distribution $P$, $\tmed(P)$, is the set of all points with maximal Tukey Depth, 
\begin{align}
	\tmed(P) = \argmax{y \in \R^d} \text{TD}_P(y) \quad .
	\label{eqn:tukey median}
\end{align}
We only access the distribution $P$ through a finite sample of i.i.d. points, $Y = \{y_1, y_2, \dots, y_n\}$. The Tukey Depth w.r.t. $Y$ is given by 
\begin{align*}
	\text{TD}_Y(y) = \inf_{w \in \R^d} |\{y' \in Y : w \cdot (y' - y) \geq 0\}| \ , 
%	\label{eqn:tukey median discrete}
\end{align*}
and the median, $\tmed(Y)$, maximizes the depth and is at most half the size of our sample $\big \lfloor \frac{n}{2} \big  \rfloor$. 

Generally, finding a point in $\tmed(Y)$ is hard; SOTA algorithms have an exponential dependency in dimension \cite{optimal_tukey}, which is a non-starter when working with high-dimensional embeddings. However, there are efficient approximations which we will take advantage of.  









