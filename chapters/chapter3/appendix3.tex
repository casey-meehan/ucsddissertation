\graphicspath{{./chapters/chapter3/}}
\chapter{ }

\label{sec:stc priv appendix} 

\subsection{Privacy Mechanism}
We now describe in detail our instance of the exponential mechanism $\mname$. Recall from Definition \ref{def: exp mech} that the exponential mechanism samples candidate $f_i \in F$ with probability
\begin{align*}
	\Pr[\calM(x) = f_i] \propto \exp\big( \frac{\epsilon u(x, f_i)}{2 \Delta u} \big) \ .
\end{align*}
Thus, $\mname$ is fully defined by its utility function, which, as listed in Equation \eqref{eqn:utility}, is approximate Tukey Depth, 
\begin{align*}
u(x, f_i) = \tdappx_{S_x}(f_i) \quad.
\end{align*}
We now describe our approximation algorithm of Tukey Depth $\tdappx_{S_x}(f_i)$, which is an adaptation of the general median hypothesis algorithm proposed by \citet{median_hyp}. 

\begin{algorithm}
    \SetKwFunction{isOddNumber}{isOddNumber}
    % \SetKwInput{Input}{Input}
    % \SetKwInput{Output}{Output}
    \SetKwInOut{KwIn}{Input}
    \SetKwInOut{KwOut}{Output}

    \KwIn{$m$ candidates $F$, \\sentence embs. $S_x = (s_1, \dots, s_k)$,\\ number of projections $p$}
    \KwOut{probability of sampling each candidate $P_F = [P_{f_1}, \dots, P_{f_m}]$}
    
    $v_1, \dots, v_p \gets $ random vecs. on unit sphere 
    
    \tcp{Project all embeddings}
  
    \For{$i \in [k]$}{
    \For{$j \in [p]$}{
    $s_i^j \gets s_i^\intercal v_j$
    }
    }
    
    \For{$i \in [m]$}{
    \For{$j \in [p]$}{
    $f_i^j \gets f_i^\intercal v_j$
    
    \tcc{Compute depth of $f_i$ on projection $v_j$}
    
    $h_j(x,f_i) \gets \#\{s_l^j : s_l^j \geq  f_i^j, l \in [k]\}$
    
    $u_j(x,f_i) \gets -\big| h_j(x,f_i) - \frac{k}{2} \big|$ 
    }
    $u(x,f_i) \gets \max_{j \in [p]} u_j(x,f_i)$
    $\hat{P}_{f_i} \gets \exp(\epsilon u(x,f_i) / 2)$
    }
    
    $\Psi \gets \sum_{i=1}^{m} \hat{P}_{f_i}$
    
    \For{$i \in [m]$}{
    $P_{f_i} \gets \frac{1}{\Psi} \hat{P}_{f_i}$
    }
    
    \KwRet{$P_F$}
    \caption{$\mname$ compute probabilities}
    \label{alg:main alg}
\end{algorithm}

Note that we can precompute the projections on line 10. The runtime is $O(mkp)$: for each of $m$ candidates and on each of $p$ projections, we need to compute the scalar difference with $k$ sentence embeddings. Sampling from the multinomial distribution defined by $P_F$ then takes $O(m)$ time. 

Additionally note from lines 13 and 15 that utility has a maximum of 0 and a minimum of $-\frac{k}{2}$, which is a semantic change from the main paper where maximum utility is $\frac{k}{2}$ and minimum is 0. 

\subsection{Proof of Privacy}

\textbf{Theorem \ref{thm:mainthm}} \emph{
	$\mname$ satisfies $\epsilon$-Sentence Privacy
}
\begin{proof}
%\begin{lemma}
%	The maximum change in the minimum $h_j$ for neighboring documents is 1. 
%	\begin{proof}
%		\max_{x, x', f_i} \big| 
%	\end{proof}
%\end{lemma}

	It is sufficient to show that the sensitivity, 
	\begin{align*}
		\Delta u = \max_{x, x', f_i} | u(x,f_i) - u(x', f_i)| \leq 1 \quad . 
	\end{align*} 
	Let us expand the above expression using the terms in Algorithm \ref{alg:main alg}. 
	\begin{align*}
		\Delta u &= \max_{x, x', f_i} | \max_{j \in [p]} u_j(x,f_i)  - \max_{j' \in [p]} u_{j'}(x',f_i)| \\ 
		&= \max_{x, x', f_i} | \min_{j \in [p]} \big| h_j(x,f_i) - \frac{k}{2} \big|  \\
		&- \min_{j' \in [p]} \big| h_{j'}(x',f_i) - \frac{k}{2} \big|| \\
		&\leq \max_{ f_i} | \min_{j \in [p]} \big| h_j(x,f_i) - \frac{k}{2} \big|  \\
		&- \big( \min_{j' \in [p]} \big| h_{j'}(x,f_i) - \frac{k}{2} \big|-1\big) | \\
		&\leq 1
	\end{align*}
	The last step follows from the fact that $|h_j(x, f_i) - h_j(x', f_i)| \leq 1$ for all $j \in [p]$. In other words, by modifying a single sentence embedding, we can only change the number of embeddings greater than $f_i^j$ on projection $j$ by 1. So, the distance of $h_j(x, f_i)$ from $\frac{k}{2}$ can only change by 1 on each projection. In the `worst case', the distance $\big| h_j(x,f_i) - \frac{k}{2} \big|$ reduces by 1 on every projection $v_j$. Even then, the minimum distance from $\frac{k}{2}$ across projections (the worst case depth) can only change by 1, giving us a sensitivity of 1. 
\end{proof}





%$\Delta u = \max_{D, D', o} | u(D,o) - u(D', o)|$. 

\subsection{Experimental Details}

Here, we provide an extended, detailed version of section \ref{sec:experiments}. 

For the general encoder, $G:\calS \rightarrow \R^{768}$, we use SBERT \cite{sbert}, a version of BERT fine-tuned for sentence encoding. Sentence embeddings are generated by mean-pooling output tokens. In all tasks, we freeze the weights of SBERT. The cluster-preserving recoder, $H$, as well as every classifier is implemented as an instance of a 4-layer MLP taking $768$-dimension inputs and only differing on output dimension. We denote an instance of this MLP with output dimension $o$ as \MLP{o}. We run 5 trials of each experiment with randomness taken over the privacy mechanisms, and plot the mean along with a $\pm$ 1 standard deviation envelope. 

\paragraph{Non-private:} For our non-private baseline, we demonstrate the usefulness of sentence-mean document embeddings. First, we generate the document embeddings $\overline{g}(x_i)$ for each training, validation, and test set document using SBERT, $G$. We then train a classifier $C_{\text{nonpriv}} = $ \MLP{r} to predict each document's topic or sentiment, where $r$ is the number of classes. The number of training epochs is determined with the validation set. 

\paragraph{\technique :} We first collect the candidate set $F$ by sampling 5k document embeddings from the subset of the training set containing at least 8 sentences. We run $k$-means with $n_c = 50$ cluster centers, and label each training set document embedding $t_i \in T_G$ with its cluster. The sentence recoder, $H = $ \MLP{768} is trained on the training set along with the linear model $L$ with the Adam optimizer and cross-entropy loss. For a given document $x$, its  sentence embeddings $S_x$ are passed through $H$, averaged together, and then passed to $L$ to predict $x$'s cluster. $L$'s loss is then back-propagated through $H$. A classifier $C_{\text{dc}} = $ \MLP{r} is trained in parallel using a separate instance of the Adam optimizer to predict class from the recoded embeddings, where $r$ is the number of classes (topics or sentiments). The number of training epochs is determined using the validation set. At test time, (generating private embeddings using $\mname$), the optimal number of projections $p$ is empirically chosen for each $\epsilon$ using the validation set. 

\paragraph{Truncation:} The truncation baseline \cite{clifton} requires first constraining the embedding instance space. We do so by computing the 75\% median interval on each of the 768 dimensions of training document embeddings $T_G$. Sentence embeddings are truncated at each dimension to lie in this box. In order to account for this distribution shift, a new classifier $C_{\text{trunc}} = $ \MLP{r} is trained on truncated mean embeddings to predict class. The number of epochs is determined with the validation set. At test time, a document's sentence embeddings $S_x$ are truncated and averaged. We then add Laplace noise to each dimension with scale factor $\frac{768 w}{k \epsilon}$, where $w$ is the width of the box on that dimension (\emph{sensitivity} in DP terms). Note that the standard deviation of noise added is inversely proportional to the number of sentences in the document, due to the averaging operation reducing sensitivity. 

\paragraph{Word Metric-DP:} Our next baseline satisfies $\epsilon$-word-level metric DP and is adopted from \cite{metricdp}. The corresponding mechanism $\text{MDP}: \calX \rightarrow \calX$ takes as input a document $x$ and returns a private version, $x'$, by randomizing each word individually. For comparison, we generate document embeddings by first randomizing the document $x' = \text{MDP}(x)$ as prescribed by \cite{metricdp}, and then computing its document embedding $\overline{g}(x')$ using SBERT. At test time, we classify the word-private document embedding using $C_{\text{nonpriv}}$. 

\paragraph{Random Guess:} To set a bottom-line, we show the theoretical performance of a random guesser. The guesser chooses class $i$ with probability $q_i$ equal to the fraction of $i$ labels in the training set. The performance is then given by $\sum_{i = 1}^{r} q_i^2$. 



\subsection{Reproducability Details}
We plan to publish a repo of code used to generate the exact figures in this paper (random seeds have been set) with the final version. Since we do not train the BERT base model $G$, our algorithms and training require relatively little computational resouces. Our system includes a single Nvidia GeForce RTX 2080 GPU and a single Intel i9 core. All of our models complete an epoch training on all datasets in less than one minute. We never do more than 20 epochs of training. All of our classifier models train (including linear model) have less than 11 million parameters. The relatively low amount of parameters is due to the fact that we freeze the underlying language model. The primary hyperparameter tuned is the number of projections $p$. We take the argmax value on the validation set between 10 and 100 projections. We repeat this for each value of $\epsilon$. 

\paragraph{Dataset preprocessing:} For all datasets, we limit ourselves to documents with at least 2 sentences. 

\imdb: This dataset has pre-defined train/test splits. We use the entire training set and form the test set by randomly sampling 4,000 from the test set provided. We do this for efficiency in computing the Metric-DP baseline, which is the slowest of all algorithms performed. Since the Metric-DP baseline randomizes first, we cannot precompute the sentence embeddings $G(s_i)$ -- we need to compute the sentence embeddings every single time we randomize. Since we randomize for each sentence of each document at each $\epsilon$ and each $k$ over 5 trials -- this takes a considerable amount of time. 

\goodreads: This dataset as provided is quite large. We randomly sample 15000 documents from each of 4 classes, and split them into 12K training examples, 2K validation examples, and 1K test examples per class. 

\tnews: We preprocess this dataset to remove all header information, which may more directly tell information about document class, and only provide the model with the sentences from the main body. We use the entire dataset, and form the Train/Val/Test splits by random sampling. 















