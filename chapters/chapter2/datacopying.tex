\section{A Test For Data-Copying}

Having provided a formal definition, we next propose a hypothesis test to detect data-copying.

\subsection{A Global Test}
\label{sec:simple-global-test}
We introduce our data-copying test in the global setting, when $\calC = \calX$. Our null hypothesis $H_0$ suggests that $Q$ may equal $P$: 
\begin{align}
    \label{eqn: null hypothesis}
     H_0: \Delta_T(P,Q) \ = \ \frac{1}{2}
\end{align}
There are well-established non-parametric tests for this hypothesis, such as the Mann-Whitney $U$ test \citep{mannwhitney}. Let $A_i \sim L(P_n), B_j \sim L(Q_m)$ be samples of $L(P), L(Q)$ given by $P_n, Q_m$ and their distances $d(X)$ to training set $T$. The $U$ statistic estimates the probability in Equation \ref{eqn: null hypothesis} by measuring the number of all $mn$ pairwise comparisons in which $B_j > A_i$. An efficient and simple method to gather and interpret this test is as follows: 
\begin{enumerate}
    \item Sort the $n+m$ values $L(P_n) \cup L(Q_m)$ such that each instance $l_i \in L(P_n), l_j \in L(Q_m)$ has rank $R(l_i), R(l_j)$, starting from rank 1, and ending with rank $n+m$. $L(P_n), L(Q_m)$ have no tied ranks with probability 1 assuming their distributions are continuous.
    \item Calculate the rank-sum for $L(Q_m)$ denoted $R_{Q_m}$, and its $U$ score denoted $U_{Q_m}$:  
    \begin{align*}
       R_{Q_m} = \sum_{l_j \in L(Q_m)} R(l_j), \quad 
       U_{Q_m} = R_{Q_m} - \frac{m(m+1)}{2}
    \end{align*}
    Consequently, $U_{Q_m} = \sum_{ij} \mathds{1}_{B_j > A_i}$. 
    \item Under $H_0$, $U_{Q_m}$ is approximately normally distributed with $>20$ samples in both $L(Q_m)$ and $L(P_n)$, allowing for the following $z$-scored statistic
    \begin{align*}
        Z_U\big(L(P_n), L(Q_m);T\big) = \frac{U_{Q_m} - \mu_U}{\sigma_U}, \\
        \mu_U = \frac{mn}{2}, \quad
        \sigma_U = \sqrt{\frac{mn(m + n + 1)}{12}}
    \end{align*}
\end{enumerate}
$Z_U$ provides us a data-copying statistic with normalized expectation and variance under $H_0$. $Z_U \ll 0$ implies data-copying, $Z_U \gg 0$ implies underfitting. $Z_U < -5$ implies that if $H_0$ holds, $Z_U$ is as likely as sampling a value $< -5$ from a standard normal.

Observe that this test is completely model agnostic and uses no estimate of likelihood. It only requires a meaningful distance metric, which is becoming common practice in the evaluation of mode-collapse and -dropping \citep{heusel, mehdi} as well.

\subsection{Handling Heterogeneity}
\label{sec:local-versus-global}
As described in Section \ref{sec:types of overfitting}, the above global test can be fooled by generators $Q$ which are very close to the training data in some regions of the instance space (overfitting) but very far from the training data in others (poor modeling). 
%Imagine collecting the $U$ statistic over the entire instance space of \textbf{Figure \ref{fig:cartoon}}. One could imagine that the (potentially underfit) samples $B_j \sim L(Q_m)$ in the green cell would cancel those from the data-copied red cell. 

% With this in mind, we formalize the failure mode of `data-copying' to be any case in which $Q$ is closer to the training set in some subspace $\calC \subseteq \calX$ than is $P$. For any distribution $D$, let $D|_{\calC}$ denote its restriction to the region $\calC$, that is, 
% \begin{align*}
%     D|_{\calC}(\calA) = \frac{D(\calA \cap \calC)}{D(\calC)} 
% \end{align*}
% for any $\calA \subseteq \X$.   
% \begin{definition}
%     A generative model $Q$ is \textit{data-copying} training set $T$ if, in some region $\calC \subseteq \calX$, it is systematically closer to $T$ by distance metric $d:\calX \rightarrow \R$ than are samples from $P$ . Specifically, if
%     \begin{align*}
%         \pr \left( B > A \ \big|\  B \sim L(Q|_\calC), A \sim L(P|_\calC) \right) \ < \ \frac{1}{2}
%     \end{align*}
% \end{definition}

% Data-copying, depicted in \textbf{Figure \ref{fig:neighbor example}}, is the type of overfitting we aim to address in this work. For completeness, we also include a definition for more standard `mode-dropping' overfitting using the same hypothesis testing framework: 

% \begin{definition}
%     A generative model $Q$ is \textit{over-representing} $P$ if in some region $\calC \subseteq \calX$, if the probability of drawing $Y \sim Q$ is much greater than it is of drawing $X \sim P$. Specifically, if 
%     \begin{align*}
%         Q(\calC) - P(\calC) \gg 0 
%     \end{align*}
% \end{definition}

We handle this by introducing a {\it local} version of our test. Let $\Pi$ denote any partition of the instance space $\calX$, which can be constructed in any manner. In our experiments, for instance, we run the $k$-means algorithm on $T$, so that $|\Pi| = k$. As the number of training and test samples grows, we may increase $k$ and thus the instance-space resolution of our test. Letting $L_\pi(D) = L(D|_{\pi})$ be the distribution of distances-to-training-set within cell $\pi \in \Pi$, we probe each cell of the partition $\Pi$ individually. 

\paragraph{Data Copying.}
\label{sec:data copying statistic}
To offer a summary statistic for data copying, we collect the $z$-scored Mann-Whitney $U$ statistic, $Z_U$, described in Section \ref{sec:simple-global-test} in each cell $\pi$. Let $P_n(\pi) = |\{x: x \in P_n, x \in \pi\}| / n$ denote the fraction of $P_n$ points lying in cell $\pi$, and similarly for $Q_m(\pi)$. The $Z_U$ test for cell $\pi$ and training set $T$ will then be denoted as $Z_U\big(L_\pi(P_n), L_\pi(Q_m); T\big)$, where $L_\pi(P_n) = \{d(x): x \in P_n, x \in \pi\}$ and similarly for $L_\pi(Q_m)$. See \textbf{Figure \ref{fig:VAE mnist neighbors}} for examples of these in-cell scores. For stability, we only measure data-copying for those cells significantly represented by $Q$, as determined by a threshold $\tau$. Let $\Pi_{\tau}$ be the set of all cells in the partition $\Pi$ for which $Q_m(\pi) \geq \tau$. Then, our summary statistic for data copying averages across all cells represented by $Q$:  

\begin{align*}
    C_T(P_n, Q_m) \coloneqq  \frac{\sum_{\pi \in \Pi_\tau} P_n(\pi) Z_U\big(L_\pi(P_n), L_\pi(Q_m); T\big)}{ \sum_{\pi \in \Pi_\tau} P_n(\pi) }
\end{align*}

\paragraph{Over-Representation.}
\label{sec:over rep statistic}
The above test will not catch a model that heavily over- or under-represents cells. For completeness, we next provide a simple representation test that is essentially used by \cite{richardson}, now with an independent test set instead of the training set.  

With $n, m\geq 20$ in cell $\pi$, we may treat $Q_m(\pi), P_n(\pi)$ as Gaussian random variables. We then check the null hypothesis $H_0 : 0 = P(\pi) - Q(\pi)$. Assuming this null hypothesis, a simple $z$-test is: 
\begin{align*}
Z_\pi = \frac{Q_m(\pi) - P_n(\pi)}
 {\sqrt{ \widehat{p}\big(1 - \widehat{p}\big) \big( \frac{1}{n} + \frac{1}{m} \big) }}
\end{align*}
where $\widehat{p} = \frac{nP_n(\pi) + mQ_m(\pi)}{n + m}$. We then report two values for a significance level $s = 0.05$: the number of significantly different cells (`bins') with $Z_\pi > s$ (NDB over-representing), and the number with $Z_\pi < -s$ (NDB under-representing).

Together, these summary statistics --- $C_T$, NDB-over, NDB-under --- detect the ways in which $Q$ broadly represents $P$ without directly copying the training set $T$. 

\subsection{Performance Guarantees}
\label{sec:perf guarantees}

We next provide some simple guarantees on the performance of the global test statistic $U(Q_m)$. Guarantees for the average test is more complicated, and is left as a direction for future work.

We begin by showing that when the null hypothesis $H_0$ does not hold, $U_{Q_m}$ has some desirable properties -- $\frac{1}{mn} U_{Q_m}$ is a consistent estimator of the quantity of interest, $\Delta_T(P,Q)$: 
\begin{thm}
    \label{thm:consistent}
    For true distribution $P$, model distribution $Q$, and distance metric $d:\calX \rightarrow \R$, the estimator $\frac{1}{mn} U_{Q_m} \rightarrow_P \Delta(P,Q)$ according to the concentration inequality 
    \begin{align*}
        \pr \big( \ \big|\frac{1}{mn} U_{Q_m} - \Delta(P,Q) \big| \geq t\big) \leq 
        \exp \bigg( -\frac{2 t^2 mn}{m + n} \bigg)
    \end{align*}
\end{thm}
Furthermore, when the model distribution $Q$ actually matches the true distribution $P$, under modest assumptions we can expect $\frac{1}{mn} U_{Q_m}$ to be near $\frac{1}{2}$: 
\begin{thm}
    \label{thm:fallback}
    If $Q = P$, and the corresponding distance distribution $L(Q) = L(P)$ is non-atomic, then
    \begin{align*}
         \E \Big[ \frac{1}{mn}U_{Q_m} \Big] &= \frac{1}{2}
         \quad \text{and} \quad \E [Z_U] = 0
    \end{align*}
\end{thm} 
Proofs are provided in Appendices \ref{sec:proof consistent} and \ref{sec:proof fallback}. 

Additionally, we show that for a Gaussian Kernel Density Estimator, the parameter $\sigma$ that satisfies the condition in Equation \ref{eqn:equal expected distance} is the $\sigma$ corresponding to a maximum likelihood Gaussian KDE model. Recall that a KDE model is described by
\begin{equation}
q_\sigma(x) = \frac{1}{(2\pi)^{k/2}|T| \sigma^k} \sum_{t \in T} \exp\left( -\frac{\|x-t\|^2}{2 \sigma^2}\right) ,
\label{eqn:kde}
\end{equation}
where the posterior probability that a random draw $x \sim q_\sigma(x)$ comes from the Gaussian component centered at training point $t$ is
\begin{align*}
    Q_\sigma(t|x) = \frac{\exp(-\|x - t\|^2/(2 \sigma^2))}{\sum_{t' \in T}\exp(-\|x - t'\|^2/(2 \sigma^2))}
\end{align*}

\begin{lemma}
For the kernel density estimator (\ref{eqn:kde}), the maximum-likehood choice of $\sigma$, namely the maximizer of $\E_{X \sim P}[\log q_\sigma(X)]$, satisfies
\begin{align*}
	\E_{X \sim P} \bigg[ \sum_{t \in T} Q_\sigma(t|X) &\|X - t\|^2 \bigg] = \\
	&\E_{Y \sim Q_\sigma} \bigg[ \sum_{t \in T} Q_\sigma(t|Y) \|Y - t\|^2 \bigg]
\end{align*}
\label{lemma:kde}
\end{lemma}
See Appendix \ref{sec:appendix kde lemma} for proof.
Unless $\sigma$ is large, we know that for any given $x \in \calX$, $\sum_{t \in T} Q_\sigma(t|x) \|x - t\|^2 \approx d(x) = \min_{t \in T} \|x - t\|^2 $. So, enforcing that $\E_{X \sim P}[d(X)] = \E_{Y \sim Q}[d(Y)]$, and more loosely that $\E_{^{A \sim L(P)}_{B \sim L(Q)}}[\mathds{1}_{B > A}] = \frac{1}{2}$ provides an excellent non-parametric approach to selecting a Gaussian KDE, and ought to be enforced for any $Q$ attempting to emulate $P$; after all, Theorem \ref{thm:fallback} points out that effectively any model with $Q = P$ also yields this condition. 
