\graphicspath{{./chapters/chapter2/}}
\chapter{ }
\subsection{Proof of Theorem \ref{thm:consistent}}
\label{sec:proof consistent}
A restatement of the theorem: 

\textit{
For true distribution $P$, model distribution $Q$, and distance metric $d:\calX \rightarrow \R$, the estimator $\frac{1}{mn} U_{Q_m} \rightarrow_P \Delta(P,Q)$ according to the concentration inequality 
\begin{align*}
    \pr \big( \ \big|\frac{1}{mn} U_{Q_m} - \Delta(P,Q) \big| \geq t\big) \leq 
    \exp \bigg( -\frac{2 t^2 mn}{m + n} \bigg)
\end{align*}
}
\begin{proof}
We establish consistency using the following nifty lemma 
\begin{lemma}{(Bounded Differences Inequality)}
\label{lem:bounded diff}
Suppose $X_1, \dots, X_n \in \calX$	are independent, and $f:\calX^n \rightarrow \R$. Let $c_1, \dots, c_n$ satisfy 
\begin{align*}
	\sup_{x_1, \dots, x_n, x_i'} \big| &f(x_1, \dots, x_i, \dots, x_n) - f(x_1, \dots, x_i', \dots, x_n) \big| \\
	&\leq c_i
\end{align*}
for $i = 1, \dots, n$. Then we have for any $t > 0$
\begin{align}
	\Pr\big(\big| f - \E[f]\big| \geq t\big) \leq \exp\bigg( \frac{-2t^2}{\sum_{i=1}^n c_i^2} \bigg) 
	\label{eqn:bounded diff}
\end{align}
\end{lemma}

This directly equips us to prove the Theorem. 

It is relatively straightforward to apply Lemma \ref{lem:bounded diff} to the normalized $\overline{U} = \frac{1}{mn} U_{Q_m}$. First, think of it as a function of $m$ independent samples of $X \sim Q$ and $n$ independent samples of $Y \sim P$, $\overline{U}(X_1, \dots, X_m, Y_1, \dots, Y_n) = \frac{1}{mn} \sum_{ij} \mathds{1}_{d(X_i) > d(Y_j)}$
\begin{align*}
	\overline{U}: (\R^{d})^{mn} \rightarrow \R 
\end{align*}
Let $b_i$ bound the change in $\overline{U}$ after substituting any $X_i$ with $X_i'$, and $c_j$ bound the change in $\overline{U}$ after substituting any $Y_j$ with $Y_j'$. Specifically 
\begin{align*}
	\sup_{x_1, \dots, x_m, y_1, \dots, y_n,\  x_i'} \big| &\overline{U}(x_1, \dots, x_i, \dots, x_m, y_1, \dots, y_n) \\
	- &\overline{U}(x_1, \dots, x_i', \dots, x_m, y_1, \dots, y_n)\big| \\
	&\leq b_i \\
	\sup_{x_1, \dots, x_m, y_1, \dots, y_n,\  y_j'} \big| &\overline{U}(x_1, \dots, x_m, y_1, \dots, y_j, \dots  y_n) \\
	- &\overline{U}(x_1, \dots, x_m, y_1, \dots, y_j', \dots  y_n)\big| \\
	&\leq c_i
\end{align*}
We then know that $b_i = \frac{n}{mn} = \frac{1}{m}$ for all $i$, with equality when $d(x_i') < d(y_j) < d(x_i)$ for all $j \in [n]$. In this case, substituting $x_i$ with $x_i'$ flips $n$ of the indicator comparisons in $\overline{U}$ from 1 to 0, and is then normalized by $mn$. By a similar argument, $c_j = \frac{m}{nm} = \frac{1}{n}$ for all $j$. 

Equipped with $b_i$ and $c_j$, we may simply substitute into Equation \ref{eqn:bounded diff} of the Bounded Differences Inequality, giving us 
\begin{align*}
	\Pr\big(\big|  \overline{U}- \E[\overline{U}]\big| \geq t\big) 
	&=  \Pr\big(\big|  \frac{1}{mn} U_{Q_m} - \Delta(\mu_p, \mu_q)\big| \geq t\big)\\
	&\leq \exp\bigg( \frac{-2t^2}{ \sum_{i=1}^m b_i^2 + \sum_{j=1}^n c_j^2} \bigg) \\
	&= \exp\bigg( \frac{-2t^2}{ \sum_{i=1}^m \frac{1}{m^2} + \sum_{j=1}^n \frac{1}{n^2} } \bigg) \\
	&= \exp\bigg( \frac{-2t^2}{ \frac{1}{m} + \frac{1}{n} } \bigg) 
	= \exp\bigg( \frac{-2t^2mn}{ m+n } \bigg)
\end{align*}
\end{proof}

\subsection{Proof of Theorem \ref{thm:fallback}}
\label{sec:proof fallback}
A restatement of the theorem: 

\textit{
When $Q = P$, and the corresponding distance distribution $L(Q) = L(P)$ is non-atomic, 
    \begin{align*}
         \E \Big[ \frac{1}{mn}\overline{U} \Big] &= \frac{1}{2}
    \end{align*}
}
\begin{proof}
	For random variables $A \sim L(P)$ and $B \sim L(P)$, we can partition the event space of $A \times B$ into three disjoint events: 
	\begin{align*}
		\Pr(A > B) &+ \Pr(A < B) \\
		&+ \Pr(A = B) = 1
	\end{align*}
	Since $Q = P$, the first two events have equal probability, $\Pr(A > B) = \Pr(A < B)$, so 
	\begin{align*}
		2\Pr(A > B) + \Pr(A = B) = 1
	\end{align*}
	And since the distributions of $A$ and $B$ are non-atomic (i.e. $\Pr\big(B = b\big) = 0, \quad \forall \ b \in \R$) we have that $\Pr(A = B) = 0$, and thus 
	\begin{align*}
		2\Pr(A > B) &= 1 \\
		\Pr(A > B) &= \Delta(P, Q) =  \frac{1}{2}
	\end{align*}
\end{proof}

\subsection{Proof of Lemma \ref{lemma:kde}} 
\label{sec:appendix kde lemma}
\textbf{Lemma \ref{lemma:kde}}
\textit{
For the kernel density estimator (\ref{eq:kde}), the maximum-likehood choice of $\sigma$, namely the maximizer of $\E_{X \sim P}[\log q_\sigma(X)]$, satisfies
\begin{align*}
	\E_{X \sim P} \bigg[ \sum_{t \in T} Q_\sigma(t|X) &\|X - t\|^2 \bigg] = \\
	&\E_{Y \sim Q_\sigma} \bigg[ \sum_{t \in T} Q_\sigma(t|Y) \|Y - t\|^2 \bigg]
\end{align*}
}
\begin{proof}
We have
\begin{align*}
&\E_{X \sim P} \left[ \ln q_\sigma(X) \right]  \\
&= \E_{X \sim P} \left[ - \ln ((2\pi)^{k/2}|T| \sigma^k) + \ln \sum_{t \in T} \exp\left( -\frac{\|x-t\|^2}{2 \sigma^2}\right)  \right] \\
&=
\mbox{constant} - k \ln \sigma + \E_{X \sim P} \left[\ln \sum_{t \in T} \exp\left( -\frac{\|x-t\|^2}{2 \sigma^2}\right) \right] 
\end{align*}
Setting the derivative of this to zero and simplifying, we find that the maximum-likelihood $\sigma$ satisfies
\begin{equation}
\sigma^2 = \frac{1}{k} \, \E_{X \sim P} \left[ \sum_{t \in T} Q_\sigma(t|X) \|X - t\|^2 \right] .
\label{eq:ml-choice}
\end{equation}
Now, interpreting $Q_\sigma$ as a mixture of $|T|$ Gaussians, and using the notation $t \in_R T$ to mean that $t$ is chosen uniformly at random from $T$, we have
\begin{align*}
	&\E_{Y \sim Q_\sigma} \left[ \sum_{t \in T} Q_\sigma(t|Y) \|Y - t\|^2 \right] \\
	&=\E_{t \in_R T} \E_{Y \sim N(t, \sigma^2 I_k)} \left[ \|Y - t\|^2 \right] 
	= k \sigma^2 .
\end{align*}
Combining this with (\ref{eq:ml-choice}) yields the lemma.
\end{proof}

\subsection{Procedural Details of Experiments}
\subsubsection{Moons Dataset, and Gaussian KDE}
\label{sec:appendix moons kDE}
\begin{figure*}[h]
    \centering
    \begin{subfigure}{.245\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/half_moon.png}
        \caption{}\label{fig:moons T}
    \end{subfigure}
        \hfill
    \begin{subfigure}{.245\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/half_moon_p01.png}
        \caption{}\label{fig:moons Q .01}
    \end{subfigure}
        \hfill
    \begin{subfigure}{.245\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/half_moon_p13.png}
        \caption{}\label{fig:moons Q .13}
    \end{subfigure}
       \hfill
    \begin{subfigure}{.245\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/half_moon_p5.png}
        \caption{}\label{fig:moons Q .5}
    \end{subfigure}
    \caption{Contour plots of KDE fit on $T$: a) training $T$ sample, b) over-fit `data copying' KDE, c) max likelihood KDE, d) underfit KDE}
    \label{fig:half moon}
\end{figure*} 

\paragraph{moons dataset}
`Moons' is a synthetic dataset consisting of two curved interlocking manifolds with added configurable noise. We chose to use this dataset as a proof of concept because it is low dimensional, and thus KDE friendly and easy to visualize, and we may have unlimited train, test, and validation samples. 

\paragraph{Gaussian KDE}
We use a Gaussian KDE as our preliminary generative model $Q$ because its likelihood is theoretically related to our non-parametric test. Perhaps more importantly, it is trivial to control the degree of data-copying with the bandwidth parameter $\sigma$. $\textbf{Figures \ref{fig:moons Q .01}, \ref{fig:moons Q .13}, \ref{fig:moons Q .5}}$ provide contour plots of of a Gaussian KDE $Q$ trained on the moons dataset with progressively larger $\sigma$. With $\sigma = 0.01$, $Q$ will effectively resample the training set. $\sigma = 0.13$ is nearly the MLE model. With $\sigma = 0.5$, the KDE struggles to capture the unique definition of $T$. 

\subsubsection{Moons Experiments}
\label{sec:appendix moons experiments}
Our experiments that examined whether several baseline tests could detect data-copying (Section \ref{sec:sensitivity to data-copying}), and our first test of our own metric (Section \ref{sec:KDE experiments}) use the moons dataset. In both of these, we fix a training sample, $T$ of 2000 points, a test sample $P_n$ of 1000 points, and a generated sample $Q_m$ of 1000 points. We regenerate $Q_m$ 10 times, and report the average statistic across these trials along with a single standard deviation. If the standard deviation buffer along the line is not visible, it is because the standard deviation is relatively small. We artificially set the constraint that $m,n \ll |T|$, as is true for big natural datasets, and more elaborate models that are computationally burdensome to sample from. 

\paragraph{Section \ref{sec:sensitivity to data-copying} Methods} Here are the routines we used for the four baseline tests: 
\begin{itemize}
    \item \textbf{Frech\'et Inception Distance (FID)} \citep{heusel}: Normally, this test is run on two samples of images ($T$ and $Q_m$) that are first embedded into a perceptually meaningful latent space using a discriminative neural net, like the Inception Network. By `meaningful' we mean points that are closer together are more perceptually alike to the human eye. Unlike images in pixel space, the samples of the moons dataset require no embedding, so we run the Frech\'et test directly on the samples. 
    
    First, we fit two MLE Gaussians: $\mathcal{N}(\mu_T, \Sigma_T)$ to $T$, and $\mathcal{N}(\mu_Q, \Sigma_Q)$ to $Q_m$, by collecting their respective MLE mean and covariance parameters. The statistic reported is the Frech\'et distance between these two Gaussians, denoted $\text{Fr}(\bullet, \bullet)$, which for Gaussians has a closed form: 
    \begin{align*}
        \text{Fr}&\big(\mathcal{N}(\mu_T, \Sigma_T), \mathcal{N}(\mu_Q, \Sigma_Q)\big) = \\
         &\|\mu_T - \mu_Q\| + \textbf{Tr}\big( \Sigma_T - \Sigma_Q - 2(\Sigma_T \Sigma_Q)^{\nicefrac{1}{2}} \big)
    \end{align*}
    Naturally, if $Q$ is data-copying $T$, its MLE mean and covariance will be nearly identical, rendering this test ineffective for capturing this kind of overfitting. 
    
    \item \textbf{Binning Based Evaluation} \citep{richardson}: This test, takes a hypothesis testing approach for evaluating mode collapse and deletion. The test bears much similarity to the test described in Section \ref{sec:local-versus-global}. The basic idea is as follows. Split the training set into partition $\Pi$ using $k$-means; the number of samples falling into each bin is approximately normally distributed if it has >20 samples. Check the null hypothesis that the normal distribution of the fraction of the training set in bin $\pi$, $T(\pi)$, equals the normal distribution of the fraction of the generated set in bin $\pi$, $Q_m(\pi)$. Specifically: 
    \begin{align*}
        Z_\pi = \frac{Q_m(\pi) - T(\pi)}
         {\sqrt{ \widehat{p}\big(1 - \widehat{p}\big) \big( \frac{1}{|T|} + \frac{1}{m} \big) }}
    \end{align*}
    where $\widehat{p} = \frac{|T|T(\pi) + mQ_m(\pi)}{|T| + m}$. We then perform a one-sided hypothesis test, and compute the number of positive $Z_\pi$ values that are greater than the significance level of 0.05. We call this the number of statistically different bins or NDB. The NDB/$k$ ought to equal the significance level if $P = Q$. 
    \item \textbf{Two-Sample Nearest-Neighbor} \citep{lopez}: In this test --- our primary baseline --- we report the three LOO NN values discussed in \cite{Kilian}. The generated sample $Q_m$ and training sample (subsampled to have equal size, $m$), $\widetilde{T} \subseteq T$, are joined together create sample $S = \widetilde{T} \cup Q_m$ of size $2m$, with training samples labeled `1' and test samples labeled `0'. One then fits a 1-Nearest-Neighbor classifier to $S$, and reports the accuracy in predicting the training samples (`1's), the accuracy in predicting the generated samples (`0's), and the average. 
    
    One can expect that --- when $Q$ collapses to a few mode centers of $T$ --- the training accuracy is low, and the generated accuracy is high, thus indicating over-representation. Additionally, one could imagine that when the training and generated accuracies are near 0, we have extreme data-copying. However, as explained in Experiments section, when we are forced to subsample $T$, it is unlikely that a given copied training point $t \in T$ is used in the test, thus making the test result unclear. 
    \item \textbf{Precision and Recall} \citep{mehdi}: This method offers a clever technique for scaling classical precision and recall statistics to high dimensional, complex spaces. First, all samples are embedded to Inception Network Pool3 features. Then, the author's use the following insight: for distribution's $Q$ and $P$, the precision and recall curve is approximately given by the set of points: 
    \begin{align*}
        \widehat{\text{PRD}}(Q,P) &= \{(\alpha(\lambda), \beta(\lambda) | \lambda \in \Lambda \}
    \end{align*}
    where
    \begin{align*}
        \Lambda &= \{ \tan\big( \frac{i}{r + 1} \frac{\pi}{2} \big) | i \in [r] \} \\
        \alpha(\lambda) &= \sum_{\pi \in \Pi} \min \big( \lambda P(\pi), Q(\pi) \big) \\
        \beta(\lambda) &= \sum_{\pi \in \Pi} \min \big( P(\pi), \frac{Q(\pi)}{\lambda} \big) 
    \end{align*}
    and where $r$ is the `resolution' of the curve, the set $\Pi$ is a partition of the instance space and $P(\pi), Q(\pi)$ are the fraction of samples falling in cell $\pi$. $\Pi$ is determined by running $k$-means on the combination of the training and generated sets. In our tests here, we set $k = 5$, and report the average PRD curve measured over 10 $k$-means clusterings (and then re-run 10 times for 10 separate trials of $Q_m$). 
    
    \end{itemize}

\subsubsection{MNIST Experiments}
\label{sec:appendix MNIST autoencoder}
\begin{figure*}[h]
	\centering
	\includegraphics[width = \linewidth]{images/latent_interp}
	\caption{Interpolating between two points in the latent space to demonstrate $L_2$ perceptual significance}
	\label{fig:latent_interp}
\end{figure*}

\begin{figure*}[h]
    \centering
    \begin{subfigure}{.31\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/moons_KDE_rep.png}
        \caption{}\label{fig:moons KDE rep}
    \end{subfigure}
        \hfill
    \begin{subfigure}{.31\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/mnist_kde_rep.png}
        \caption{}\label{fig:mnist kde rep}
    \end{subfigure}
        \hfill
    \begin{subfigure}{.31\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/VAE_rep.jpg}
        \caption{}\label{fig:vae rep}
    \end{subfigure}
    \caption{Number of statistically different bins, both those over and under the significance level of 0.05. The black dotted line indicates the total number of cells or `bins'. \textbf{(a,b)} KDEs tend to start misrepresenting with $\sigma \gg \sigma_{\text{MLE}}$, which makes sense as they become less and less dependent on training set. \textbf{(c)} it makes sense that the VAE over- and under-represents across all latent dimensions due to its reverse KL loss. There is slightly worse over- and under-representation for simple models with low latent dimension. }
    \label{fig:rep tests 1}
\end{figure*} 

\begin{figure*}[h]
    \centering
    \begin{subfigure}{.31\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/biggan_coffee_rep.png}
        \caption{}\label{fig:moons KDE rep}
    \end{subfigure}
        \hfill
    \begin{subfigure}{.31\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/biggan_bubble_rep.png}
        \caption{}\label{fig:mnist kde rep}
    \end{subfigure}
        \hfill
    \begin{subfigure}{.31\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/biggan_schooner_rep.png}
        \caption{}\label{fig:vae rep}
    \end{subfigure}
    \caption{This GAN model produces relatively equal representation according to our clustering for all three classes. It makes sense that a low truncation level tends to over-represent for one class, as a lower truncation threshold causes less variance. Even though it places samples into all cells, some cells are data-copying much more aggressively than others.}
    \label{fig:biggan rep tests }
\end{figure*} 
The experiments of Sections \ref{sec:MNIST KDE} and \ref{sec:neural model tests} use the MNIST digit dataset \citep{lecun}. We use a training sample, $T$, of size $|T| = 50,000$, a test sample $P_n$ of size $n = 10,000$, a validation sample $V_l$ of $l = 10,000$, and create generated samples of size $m = 10,000$. 

Here, for a meaningful distance metric, we create a custom embedding using a convolutional autoencoder trained using a VGG perceptual loss proposed by \cite{zhang}. The encoder and decoder each have four convolutional layers using batch normalization, two linear layers using dropout, and two max pool layers. The autoencoder is trained for 100 epochs with a batch size of 128 and Adam optimizer with learning rate 0.001. For each training sample $t \in \R^{784}$, the encoder compresses to $z \in \R^{64}$, and decoder expands back up to $\widehat{t} \in \R^{784}$. Our loss is then 
\begin{align*}
    L(t, \widehat{t}) = \gamma(t, \widehat{t}) +  \lambda  \max \{\| z \|_2^2 - 1, 0  \}
\end{align*}
where $\gamma(\bullet, \bullet)$ is the VGG perceptual loss, and $ \lambda  \max \{\| z \|_2^2 - 1, 0  \}$ provides a linear hinge loss outside of a unit $L_2$ ball. The hinge loss encourages the encoder to learn a latent representation within a bounded domain, hopefully augmenting its ability to interpolate between samples. It is worth noting that the perceptual loss is not trained on MNIST, and hopefully uses agnostic features that help keep us from overfitting. We opt to use a standard autoencoder instead of a stochastic autoencoder like a VAE, because we want to be able to exactly data-copy the training set $T$. Thus, we want the encoder to create a near-exact encoding and decoding of the training samples specifically. \textbf{Figure \ref{fig:latent_interp}} provides an example of linearly spaced steps between two training samples. While not perfect, we observe that half-way between the `2' and the `0' is a sample that appears perceptually to be almost almost a `2' and almost a `0'. As such, we consider the distance metric $d(x)$ on this space used in our experiments to be meaningful. 

\begin{figure*}[h]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/biggan_schooner_overfit_cluster.png}
        \label{fig:biggan schooner overfit}
        \caption{$Z_U = -1.05$}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/biggan_schooner_underfit_cluster.png}
        \label{fig:biggan schooner underfit}
        \caption{$Z_U = +1.32$}
    \end{subfigure}
    \caption{Example of data-copied and underfit cell of ImageNet `schooner' instance space, from `BigGan' with trunc. threshold = 2. We note here, that --- limited to only 50 training samples --- the insufficient $k = 3$ clustering is perhaps not fine grain enough for this class. Notice that the generated samples falling into the underfit cell (mostly training images of either masts or fronts of boats) are hardly any different from those of the over-fit cell.  They are likely on the boundary of the two cells. With that said, the samples of the data-copied cell \textbf{(a)} are certainly close to the training samples in this region.}
    \label{fig:biggan schooner examples}
\end{figure*}

\paragraph{\textbf{KDE tests}:}
In the MNIST KDE experiments, we fit each KDE $Q$ on the 64-d latent representations of the training set $T$ for several values of $\sigma$; we gather all statistical tests in this space, and effectively only decode to visaully inspect samples. We gather the average and standard deviation of each data point across 5 trials of generating $Q_m$. For the Two-Sample Nearest-Neighbor test, it is computationally intense to compute the nearesnt neighbor in a 64-dimensional dataset of 20,000 points $\widetilde{T} \cup Q_m$ 20,000 times. To limit this, we average each of the training and generated NN accuracy over 500 training and generated samples. We find this acceptable, since the test results depicted in \textbf{Figure \ref{fig:mnist kde NN}} are relatively low variance. 

\paragraph{VAE experiments:}
\label{sec:appendix VAE experiments}
In the MNIST VAE experiments, we only use the 64-d autoencoder latent representation in computing the $C_T$ and 1-NN test scores, and not at all in training. Here, we experiment with twenty standard, fully connected, VAEs using binary cross entropy reconstruction loss. The twenty models have three hidden layers and latent dimensions ranging from $d = 5$ to $d = 100$ in steps of 5. The number of neurons in intermediate layers is approximately twice the number of the layer beneath it, so for a latent space of 50-d, the encoder architecture is $784 \rightarrow 400 \rightarrow 200 \rightarrow 100 \rightarrow 50$, and the decoder architecture is the opposite. 

To sample from a trained VAE, we sample from a standard normal with dimensionality equivalent to the VAEs latent dimension, and pass them through the VAE decoder to the 784-d image space. We then encode these generated images to the agnostic 64-d latent space of the perceptual autoencoder described at the beginning of the section, where $L_2$ distance is meaningful. We also encode the training sample $T$ and test sample $P_n$ to this space, and then run the $C_T$ and two-sample NN tests. We again compute the nearest neighbor accuracies for 500 of the training and generated samples (the 1-NN classifier is fit on the 20,000 sample set $\widetilde{T} \cup Q_m$), which appears to be acceptable due to low test variance. 

\subsubsection{ImageNet Experiments}
\label{sec:appendix biggan experiments}
Here, we have chosen three of the one thousand ImageNet12 classes that `BigGan' produces. To reiterate, a conditional GAN can output samples from a specific class by conditioning on a class code input. We acknowledge that conditional GANs combine features from many classes in ways not yet well understood, but treat the GAN of each class as a uniquely different generative model trained on the training samples from that class. So, for the `coffee' class, we treat the GAN as a coffee generator $Q$, trained on the 1300 `coffee' class samples. For each class, we have 1300 training samples $|T|$, 2000 generated samples $m$, and 50 test samples $n$. Being atypically training sample starved ($m > |T|$), we subsample $Q_m$ (not $T$!), to produce equal size samples for the two-sample NN test. As such, all training samples used are in the combined set $S$. We also note that the 50 test samples provided in each class is highly limiting, only allowing us to split the instance space into about three cells and keep a reasonable number of test samples in each cell. As the number of test samples grows, so can the number of cells and the resolution of the partition. \textbf{Figure \ref{fig:biggan schooner examples}} provides an example of where this clustering might be limited; the generated samples of the underfit cell seem hardly any different from those of the over-fit cell. A finer-grain partition is likely needed here. However, the data-copied cell to the left does appear to be very close to the training set, potentially too close according to $Z_U$. 

In performing these experiments, we gather the $C_T(P_n, Q_m)$ statistic for a given class of images. In an attempt to embed the images into a lower dimensional latent space with $L_2$ significance, we pass each image through an InceptionV3 network and gather the 2048-dimension feature embeddings after the final average pooling layer (Pool3). We then project all inception-space images ($T, P_n, Q_m$) onto the 64 principal components of the training set embeddings. Finally, we use $k$-means to partition the points of each sample into one of $k = 3$ cells. The number of cells is limited by the 50 test images available per class. Any more cells would strain the Central Limit Theorem assumption in computing $Z_U$. Finally, we gather the $C_T$ and two-sample NN baseline statistics on this 64-d space. 

\begin{figure*}[h]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/VAE_C_T_vs_d.png}
        \caption{$C_T(P_n, Q_m)$}
        \label{fig:kMMD comparison vae}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/VAE_kMMD_XZ_vs_d.png}
        \caption{kernel MMD}
        \label{fig:kMMD comparison kMMD}
    \end{subfigure}
    \caption{Comparison of the $C_T(P_n, Q_m)$ test presented in this paper alongside the three sample kMMD test. }
    \label{fig:kMMD comparison}
\end{figure*}

\subsection{Comparison with three-sample Kernel-MMD:}
Another three-sample test not shown in the main body of this work is the three-sample kernel MMD test introduced by \cite{gretton_2} intended more for model comparison than for checking model overfitting. For samples $X \sim P$ and $Y \sim Q$, we can estimate the squared kernel MMD between $P$ and $Q$ under kernel $k$ by empirically estimating 

\begin{align*}
    &\text{MMD}^2(P,Q) \\
    &= \E_{x,x' \sim P}[k(x,x')] - 2\E_{x\sim P, y \sim Q}[k(x,y)] +  \E_{y,y' \sim Q}[k(y,y')]
\end{align*}

More recent works such as \cite{reviewer_paper} have repurposed this test for measuring generative model overfitting. Intuitively, if the model is overfitting its training set, the empirical kMMD between training and generated data may be smaller than that between training and test sets. This may be triggered by the data-copying variety of overfitting. 

This test provides an interesting benchmark to consider in addition to those in the main body. \textbf{Figure \ref{fig:kMMD comparison}} demonstrates some preliminary experimental results repeating the MNIST VAE experiment \textbf{Figure \ref{fig:vae C_T vs d}}. To implement the kMMD, we used code posted by \cite{gretton_2} https://github.com/eugenium/MMD, and ran the three sample RBF-kMMD test on twenty MNIST VAEs with decreasing complexity (latent dimension). \textbf{Figure \ref{fig:kMMD comparison kMMD}} attached plots the kMMD distance to training set for both the generated (orange) and test samples (blue). We observe that this test does not appear sensitive to over-parametrized VAEs ($d > 50$) in the same way our proposed test (Figure 1a attached) is. It is sensitive to underfitting ($d << 50$), however. The $p$-values of that work's corresponding hypothesis test (figure not shown) similarly did not respond to data-copying.
We suspect that this insensitivity to data-copying is due to the global nature of this test, incapable of capturing data-copying at smaller scales. 
