\section{Experiments}
\label{sec:experiments}
\begin{figure*}
    \centering
    \begin{subfigure}{.24\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/compare_models_frechet.png}
        \caption{}\label{fig:moons frechet}
    \end{subfigure}
    \begin{subfigure}{0.24\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/compare_methods_binning.png}
        \caption{}\label{fig:moons binning}
    \end{subfigure}
    % \vskip\baselineskip
    \begin{subfigure}{.24\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/compare_methods_NN.png}
        \caption{}\label{fig:moons NN compare}
    \end{subfigure}
    \begin{subfigure}{0.24\linewidth}
        \centering
        % \begin{overpic}[width = 1\linewidth]{images/compare_methods_PR.png}
        % \put(15,15){
        \includegraphics[width = 1\linewidth]{images/compare_methods_PR_zoom.png}
        % }
        % \end{overpic}
        \caption{}\label{fig:moons PR}
    \end{subfigure}
    \caption[Response of four baseline test methods to data-copying of a Gaussian KDE on `moons' dataset.]{Response of four baseline test methods to data-copying of a Gaussian KDE on `moons' dataset. Only the two-sample NN test \textbf{(c)} is able to detect data-copying KDE models as $\sigma$ moves below $\sigma_{\text{MLE}}$ (depicted as a red dot). The gray trace is proportional to the KDE's log-likelihood measured on a held-out validation set.}
    \label{fig:compare methods}
\end{figure*} 

Having clarified what we mean by data-copying in theory, we turn our attention to data copying by generative models in practice. We leave representation test results for the appendix, since this behavior has been well studied in previous works. Specifically, we aim to answer the two following questions:
\begin{enumerate}
    \item  Are the existing tests that measure generative model overfitting able to capture data-copying? 
    \item As popular generative models range from over- to underfitting, does our test indicate data-copying, and if so, to what degree? 
\end{enumerate}

\paragraph{Training, Generated and Test Sets.}
In all of the following experiments, we select a training dataset $T$ with test split $P_n$, and a generative model $Q$ producing a sample $Q_m$. We perform $k$-means on $T$ to determine partition $\Pi$, with the objective having a reasonable population of both $T$ and $P_n$ in each $\pi \in \Pi$. We set threshold $\tau$, such that we are guaranteed to have at least 20 samples in each cell in order to validate the gaussian assumption of $Z_\pi, Z_U$. 

%We then select some parameter of $Q$ that can tune the degree of over- or underfitting on training set $T$. For instance, the Gaussian KDE $\sigma$ parameter will directly control the degree of data-copying by our definition, allowing us to sweep $\sigma$ from low (complex, over-fit model) to high (simple, underfit model). For VAEs, we have no such parameter, and instead vary the model complexity from high (many units per layer) to low (few units per layer). We then probe for the degree of data-copying at each level of declining model complexity using the baseline and proposed methods, and record test responses. To observe the variance of each test, we record the average and 1-standard deviation of the test response across several trials of generating $Q_m$.

%We embed all image samples into some latent space with meaningful $L_2$ distance to make $d(x)$ significant. While this is standard practice in evaluating generative models \citep{salimans, mehdi}, these embeddings themselves might be overfit. To address this, we perform our experiments in three domains with three different kinds of embeddings (none, custom, and Inception Network Pool3 features). 

\subsection{Detecting data-copying}
\label{sec:sensitivity to data-copying}

First, we investigate which of the existing generative model tests can detect explicit data-copying.

\paragraph{Baselines and Dataset} Here, we probe the four of the methods described in our Related Work section, to see how they react to data-copying: two-sample NN \citep{lopez}, FID \citep{heusel}, Binning-Based Evaluation \citep{richardson}, and Precision \& Recall \citep{mehdi}, which are described in detail in Appendix \ref{sec:appendix moons experiments}. We run this test on the two-dimensional `moons' dataset, as it affords us limitless training and test samples and requires no feature embedding (see Appendix \ref{sec:appendix moons kDE} for an example). Note that, without an embedding, FID is simply the Frech\'et distance between two MLE normal distributions fit to $T$ and $Q_m$. We use the same size generated and training sample for all methods, when $m < |T|$ (especially for large datasets and computationally burdensome samplers) we are forced to use an $m$-size training subsample $\widetilde{T}$ for running the two-sample NN test due to its constraint that $m = |T|$.  

The canonical method of measuring the generalization gap (difference between training and test set likelihoods under the model) is not one of our primary baselines due to the fact that it cannot scale to contemporary models with intractable likelihoods (e.g. GANs / VAEs). It is, however, included for reference in Figure \ref{fig:KDE results}. While this method naturally exposes data-copying, it is generally insensitive to underfitting: $\Delta(P,Q) > \frac{1}{2}$. 

We make $Q$ a Gaussian KDE since, as described in Section \ref{sec:perf guarantees}, it allows us to force explicit data-copying by setting $\sigma$ very low. As $\sigma \rightarrow 0$, $Q$ becomes a bootstrap sampler of the original training set. If a given test method can detect the level of data-copying by $Q$ on $T$, it will provide a different response to a heavily over-fit KDE $Q$ ($\sigma \ll \sigma_{\text{MLE}}$), a well-fit KDE $Q$ ($\sigma \approx \sigma_{\text{MLE}}$), and an underfit KDE $Q$ ($\sigma \gg \sigma_{\text{MLE}}$). 

\textbf{Figure \ref{fig:compare methods}} depicts how each baseline method responds to KDE $Q$ models of varying degrees of data-copying, as $Q$ ranges from data-copying ($\sigma = 0.001$) up to heavily underfit ($\sigma = 10$). The Frech\'et and Binning methods report effectively the same value for all $\sigma \leq \sigma_{\text{MLE}}$, indicating inability to detect data-copying. Similarly, the PR curves for different $\sigma$ values are high variance and show no meaningful order with respect to $\sigma$. 

The two-sample NN test does show a mild change in response as $\sigma$ decreases below $\sigma_{\text{MLE}}$. This makes sense; as points in $Q_m$ become closer to points in $T$, the two-sample NN accuracy should steadily drop to zero. The reason it does not drop to zero is due to the $m$ subsampled training points, $\widetilde{T} \subset T$, needed to perform this test. As such, each training point $t \in T$ being copied by generated point $q \in Q_m$ is unlikely to be present in $\widetilde{T}$ during the test. This phenomenon is especially pronounced in some of the following settings. 

The reason most of these tests fail to detect data-copying is because most existing methods focus on another type of overfitting: mode-collapse and -dropping, wherein entire modes of $P$ are either forgotten or averaged together. However, if a model begins to data-copy, it is definitively overfitting \emph{without} mode-collapsing. 

Next, we will demonstrate our method on a variety of datasets, models, and embeddings. We will compare our method to the two-sample NN method in each setting, as it is the only baseline that responds to explicit data-copying.

\begin{figure*}
    \centering
    \begin{subfigure}{.27\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/moons_gen_gap.png}
        \caption{}\label{fig:moons gen gap}
    \end{subfigure}
        \hfill 
    \begin{subfigure}{.27\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/moons_z_score.png}
        \caption{}\label{fig:moons z score}
    \end{subfigure}
        \hfill
    \begin{subfigure}{.27\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/moons_NN.png}
        \caption{}\label{fig:moons NN}
    \end{subfigure} 
        \vskip\baselineskip
        % \hfill
    \begin{subfigure}{.27\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/mnist_kde_gen_gap.png}
        \caption{}\label{fig:mnist gen gap}
    \end{subfigure}
        \hfill 
    \begin{subfigure}{.27\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/mnist_kde_M_tilde_score.png}
        \caption{}\label{fig:mnist kde z score}
    \end{subfigure}
        \hfill
    \begin{subfigure}{.27\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/mnist_kde_NN.png}
        \caption{}\label{fig:mnist kde NN}
    \end{subfigure}
    \caption[$C_T(P_n, Q_m)$ vs. NN baseline and generalization gap on moons and MNIST digits datasets.]{$C_T(P_n, Q_m)$ vs. NN baseline and generalization gap on moons and MNIST digits datasets. \textbf{(a,b,c)} compare the three methods on the moons dataset. \textbf{(d,e,f)} compare the three methods on MNIST. In both data settings, the $C_T$ statistic is far more sensitive to the data-copying regime $\sigma \ll \sigma_{\text{MLE}}$ than the NN baseline. It is more sensitive to underfitting $\sigma \gg \sigma_{\text{MLE}}$ than the generalization gap test. The red dot denotes $\sigma_{\text{MLE}}$, and the gray trace is proportional to the KDE's log-likelihood measured on a held-out validation set.}
    \label{fig:KDE results}
\end{figure*} 

\subsection{Measuring degree of data-copying}
We now aim to answer the second question raised at the beginning of this section: does $C_T(P_n, Q_m)$ detect and quantify data-copying? We focus on two types of generative model: Gaussian KDEs, and neural models.

\subsubsection{KDE-based tests}
\label{sec:KDE experiments}
 While KDEs do not provide a reliable likelihood in high dimension \citep{theis}, they do provide an informative first benchmark of the $C_T$ statistic. KDEs allow us to directly force data-copying, and confirm the theoretical connection between the MLE KDE and $C_T \approx 0$ described in Lemma \ref{lemma:kde}.

\paragraph{KDEs: `moons' dataset}

Here, we repeat the experiment performed in Section \ref{sec:sensitivity to data-copying}, now including the $C_T$ statistic for comparison. Refer to Appendix \ref{sec:appendix moons kDE} for experimental details, and examples of the dataset. For reference, \textbf{Figure \ref{fig:moons gen gap}} depicts how the generalization gap dwindles as KDE $\sigma$ increases. While this test is capable of capturing data-copying, it is insensitive to underfitting and relies on a tractable likelihood.

\textbf{Figures \ref{fig:moons z score}} and \textbf{ \ref{fig:moons NN}} give a side-by-side depiction of $C_T$ and the two-sample NN test accuracies across a range of KDE $\sigma$ values. Think of $C_T$ values as $z$-score standard deviations. We see that the $C_T$ statistic in \textbf{Figure \ref{fig:moons z score}} precisely identifies the MLE model when $C_T \approx 0$, and responds sharply to $\sigma$ values above and below $\sigma_{\text{MLE}}$. The baseline in \textbf{Figure \ref{fig:moons NN}} similarly identifies the MLE $Q$ model when training accuracy $\approx 0.5$, but is higher variance and less sensitive to changes in $\sigma$, especially for over-fit $\sigma \ll \sigma_{\text{MLE}}$. We will see in the next experiment, that this test breaks down for more complex datasets when $m \ll |T|$. 

\paragraph{KDEs: MNIST Handwritten Digits}
\label{sec:MNIST KDE}
We now extend the KDE test performed on the moons dataset to the significantly more complex MNIST handwritten digit dataset \citep{lecun}. 

While it would be convenient to directly apply the KDE $\sigma$-sweeping tests discussed in the previous section, there are two primary barriers. The first is that KDE model relies on $L_2$ norms being perceptually meaningful, which is well understood not to be true in pixel space. The second problem is that of dimensionality: the 784-dimensional space of digits is far too high for a KDE to be even remotely efficient at interpolating the space. 

To handle these issues, we first embed each image, $x \in \calX$, to a perceptually meaningful 64-dimensional latent code, $z \in \mathcal{Z}$. We achieve this by training a convolutional autoencoder with a VGGnet perceptual loss produced by \cite{zhang} (see Appendix \ref{sec:appendix MNIST autoencoder} for more detail). Surely, even in the lower 64-dimensional space, the KDE will suffer some from the curse of dimensionality. We are not promoting this method as a powerful generative model, but rather as an instructive tool for probing a test's response to data-copying in the image domain. 

Again, the likelihood generalization gap is depicted in \textbf{Figure \ref{fig:mnist gen gap}} repeating the trend seen with the `moons' dataset. 
% Comparing the $C_T(P_n, Q_m)$ statistic to the two-sample NN baseline. 
Here, we run all tests in the compressed latent space. See Appendix \ref{sec:appendix MNIST autoencoder} for experimental details. 

\textbf{Figure \ref{fig:mnist kde z score}} shows how $C_T(P_n, Q_m)$ reacts decisively to over- and underfitting. It falsely determines the MLE $\sigma$ value as slightly over-fit. However, the region of where $C_T$ transitions from over- to underfit (say $-13 \leq C_T \leq 13$) is relatively tight and includes the MLE $\sigma$.

Meanwhile, \textbf{Figure \ref{fig:mnist kde NN}} shows how --- with the generated sample smaller than the training sample, $m \ll |T|$ --- the two-sample NN baseline provides no meaningful estimate of data-copying. In fact, the most data-copying models with low $\sigma$ achieve the best scores closest to $0.5$. Again, we are forced to use the $m$-subsampled $\widetilde{T} \subset T$, and most instances of data copying are completely missed. $C_T$ has no such restriction.

These results are promising, and demonstrate the reliability of this hypothesis testing approach to probing for data-copying across different data domains. In the next section, we explore how these tests perform on more sophisticated, non-KDE models. 

\subsection{Neural Model Tests}
\label{sec:neural model tests}
Gaussian KDE's may have nice theoretical properties, but are relatively ineffective in high-dimensional settings, precluding domains like images. As such, we also demonstrate our experiments on more practical neural models trained on higher dimensional image datasets (MNIST and ImageNet), with the goal of observing whether the $C_T$ statistic indicates data-copying as these models range from over- to underfit. 

\paragraph{MNIST VAE}
\begin{figure*}[h]
    \centering
    \begin{subfigure}{.27\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/VAE_gg_vals.png}
        \caption{}\label{fig:vae gen gap}
    \end{subfigure}
    \hfill 
    \begin{subfigure}{.27\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/VAE_C_T_vs_d.png}
        \caption{}\label{fig:vae C_T vs d}
    \end{subfigure}
        \hfill
    \begin{subfigure}{.27\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/VAE_NN_vs_d.png}
        \caption{}\label{fig:vae NN vs d}
    \end{subfigure}
    % \vskip\baselineskip
        \hfill
    \begin{subfigure}{.27\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/biggan_C_T_all.png}
    \caption{}\label{fig:biggan C_T score}
    \end{subfigure}
        \quad 
    \begin{subfigure}{.27\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/biggan_NN_all.png}
        \caption{}\label{fig:biggan NN}
    \end{subfigure}
    \caption[Neural model data-copying: figures \textbf{(b)} and \textbf{(d)} demonstrate the $C_T$ statistic identifying data-copying in an MNIST VAE and ImageNet GAN as they range from heavily over-fit to underfit.]{Neural model data-copying: figures \textbf{(b)} and \textbf{(d)} demonstrate the $C_T$ statistic identifying data-copying in an MNIST VAE and ImageNet GAN as they range from heavily over-fit to underfit. \textbf{(c)} and \textbf{(e)} demonstrate the relative insensitivity of the NN baseline to this overfitting, as does figure \textbf{(a)} of the generalization (ELBO) gap method for VAEs. (Note, the markers for \textbf{(d)} apply to the traces of \textbf{(e)})}
    \label{fig:neural experiments}
\end{figure*}

\begin{figure*}[h]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/biggan_coffee_overfit_cluster.png}
        \label{fig:biggan coffee overfit}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/biggan_coffee_underfit_cluster.png}
        \label{fig:biggan coffee underfit}
    \end{subfigure}
    % \vskip\baselineskip
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/biggan_bubble_overfit_cluster.png}
        \label{fig:biggan bubble overfit}
        \caption{Data-copied cells; top: $Z_U = -1.46$, bottom: $Z_U = -1.00$} 
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width = 1\linewidth]{images/biggan_bubble_underfit_cluster.png}
        \label{fig:biggan bubble underfit}
        \caption{Underfit cells; top: $Z_U = +1.40$, bottom: $Z_U = +0.71$} 
    \end{subfigure}
    \caption[Data-copied and underfit cells of ImageNet12 `coffee' and `soap bubble' instance spaces (trunc. threshold = 2).]{Data-copied and underfit cells of ImageNet12 `coffee' and `soap bubble' instance spaces (trunc. threshold = 2). In each 14-figure strip, the top row provides a random series of training samples from the cell, and the bottom row provides a random series of generated samples from the cell. \textbf{(a)} Data-copied cells. \textbf{(a), top}: Random training and generated samples from a $Z_U = -1.46$ cell of the coffee instance space. \textbf{(a), bottom}: Random training and generated samples from a $Z_U = -1.00$ cell of the bubble instance space. \textbf{(b)} Underfit cells. \textbf{(b), top}: Random training and generated samples from a $Z_U = +1.40$ cell of the coffee instance space. \textbf{(b), bottom}: Random training and generated samples from a $Z_U = +0.71$ cell of the bubble instance space.  }
    \label{fig:biggan overfit underfit}
\end{figure*}

Here, we employ our data-copying test, $C_T(P_n, Q_m)$, on a range of VAEs of varying complexity trained on the MNIST handwritten digit dataset. Experimental and theoretical findings have suggested that VAE samplers --- under certain assumptions --- simply produce convex combinations of training set samples \citep{VAEs_overfit}. In generating an out-of-distribution sample, an overly complex VAE effectively reproduces nearest-neighbor training samples. Our findings appear to corroborate this. We vary model complexity by increasing the width (neurons per layer) in a three-layer VAE (see Appendix \ref{sec:appendix VAE experiments} for details). As an embedding, we pass all samples through the the convolutional autoencoder of Section \ref{sec:MNIST KDE}, and collect statistics in this 64-dimensional space.  

 \textbf{Figures \ref{fig:vae C_T vs d}} and \textbf{\ref{fig:vae NN vs d}} compare the $C_T$ statistic to the NN accuracy baseline . $C_T$ behaves as it did in the previous sections: more complex models over-fit, forcing $C_T \ll 0$, and less complex models underfit forcing it $\gg0$. We note that the range of $C_T$ values is far less dramatic, which is to be expected since the KDEs were forced to explicitly data-copy. As likelihood is not available for VAEs, we compute each model's ELBO on a 10,000 sample held out validation set, and plot it in gray. We observe that the ELBO spikes for models with $C_T$ near 0. \textbf{Figure \ref{fig:vae gen gap}} shows the ELBO approximation of the generalization gap as the latent dimension (and number of units in each layer) is decreased. This method is entirely insensitive to over- and underfit models. This may be because the ELBO is only a lower bound, not the actual likelihood. 

The NN baseline in \textbf{Figure \ref{fig:vae NN vs d}} is less interpretable, and fails to capture the overfitting trend as $C_T$ does. While all three test accuracies still follow the upward-sloping trend of \textbf{Figure \ref{fig:moons NN}}, they do not indicate where the highest validation set ELBO is. Furthermore, the NN accuracy statistics are shifted upward when compared to the results of the previous section: all NN accuracies are above 0.5 for all latent dimensions. This is problematic. A test statistic's absolute score ought to bear significance between very different data and model domains like KDEs and VAEs.

\paragraph{ImageNet GAN}

Finally, we scale our experiments up to a more practical image domain. We gather our test statistics on a state of the art conditional GAN, `\emph{BigGan}' \citep{BigGan}, trained on the Imagenet 12 dataset \citep{imagenet12}. Conditioning on an input code, this GAN will generate one of 1000 different Imagenet classes. We run our experiments separately on three classes: `coffee', `soap bubble', and `schooner'. All generated, test, and training images are embedded to a 64-dimensional space by first gathering the 2048-dimensional features of an InceptionV3 network `Pool3' layer, and then projecting them onto the 64 principal components of the training embeddings. Appendix \ref{sec:appendix biggan experiments} has more details. 

Being limited to one pre-trained model, we increase model variance (`truncation threshold') instead of decreasing model complexity. As proposed by \emph{BigGan}'s authors, all standard normal input samples outside of this truncation threshold are resampled. The authors suggest that lower truncation thresholds, by only producing samples at the mode of the input, output higher quality samples at the cost of variety, as determined by Inception Score (IS). Similarly, the FID score finds suitable variety until truncation approaches zero. 

As depicted in \textbf{ Figure \ref{fig:biggan C_T score}}, the $C_T$ statistic remains well below zero until the truncation threshold is nearly maximized, indicating that $Q$ produces samples closer to the training set than real samples tend to be. While FID finds that \emph{in aggregate} the distributions are roughly similar, a closer look suggests that $Q$ allocates too much probability mass near the training samples. 

Meanwhile, the two-sample NN baseline in \textbf{ Figure \ref{fig:biggan NN} } hardly reacts to changes in truncation, even though the generated and training sets are the same size, $m = |T|$. Across all truncation values, the training sample NN accuracy remains around 0.5, not quite implying over- or underfitting.  

A useful feature of the $C_T$ statistic is that one can examine the $Z_U$ scores it is composed of to see which of the cells $\pi \in \Pi_{\tau}$ are or are not copying. \textbf{ Figure \ref{fig:biggan overfit underfit} } shows the samples of over- and underfit clusters for two of the three classes. For both `coffee' and `bubble' classes, the underfit cells are more diverse than the data-copied cells. While it might seem reasonable that these generated samples are further from nearest neighbors in more diverse clusters, keep in mind that the $Z_U > 0$ statistic indicates that they are further from training neighbors than test set samples are. For instance, the people depicted in underfit `bubbles' cell are highly distorted.  

\section{Conclusion}
In this work, we have formalized \emph{data-copying}: an under-explored failure mode of generative model overfitting. We have provided preliminary tests for measuring data-copying and experiments indicating its presence in a broad class of generative models. In future work, we plan to establish more theoretical properties of data-copying, convergence guarantees of these tests, and experiments with different model parameters. 

\section{Acknowledgements}
We thank Rich Zemel for pointing us to \cite{Kilian}, which was the starting point of this work. Thanks to Arthur Gretton and Ruslan Salakhutdinov for pointers to prior work, and Philip Isola and Christian Szegedy for helpful advice. Finally, KC and CM would like to thank  ONR under N00014-16-1-2614,  UC Lab Fees under LFR 18-548554 and NSF IIS 1617157 for research support.

This chapter in full, is a reprint of the material as it appears in International Conference on Artificial Intelligence and Statistics, 2020. Casey Meehan, Sanjoy Dasgupta, Kamalika Chaudhuri. \emph{A Non-parametric Test to Detect Data-Copying in Generative Models}. The dissertation author is the primary investigator and author of this paper. 

